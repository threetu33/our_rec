#!/usr/bin/env python3
"""
å®Œå…¨ä¿®å¤ç‰ˆæœ¬çš„R1å…¼å®¹æµ‹è¯•è„šæœ¬
ä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ—¶çš„è¯„ä¼°æµç¨‹è¿›è¡Œæ¨ç†
"""

import os
import sys
import json
import torch
import random
import numpy as np
from tqdm import tqdm
from typing import List, Dict, Any, Optional
from datasets import load_from_disk
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/data2/home/hongdeyao/RRec')

from paths import model_names
from trainers.utils import get_tokenizer, calculate_metrics, Similarity
from peft import PeftModel
from prompters.rrec_prompter import UserGenPrompter, UserPrompter
from prompters.prompts import obtain_prompts

class TrainingConsistentTester:
    def __init__(self, 
                 checkpoint_path: str,
                 dataset_dir: str,
                 model_type: str = "qwen",
                 device: str = "cuda:6",
                 reference_json_path: Optional[str] = None,
                 user_window_size: int = 10):
        """
        åˆå§‹åŒ–è®­ç»ƒä¸€è‡´æ€§æµ‹è¯•å™¨
        
        Args:
            checkpoint_path: checkpointè·¯å¾„
            dataset_dir: æ•°æ®é›†ç›®å½•
            model_type: æ¨¡å‹ç±»å‹
            device: ä½¿ç”¨çš„è®¾å¤‡
            reference_json_path: å‚è€ƒJSONæ–‡ä»¶è·¯å¾„,åŒ…å«å›ºå®šçš„user_idå’Œcandidate_ids
        """
        self.checkpoint_path = checkpoint_path
        self.dataset_dir = dataset_dir
        self.model_type = model_type
        self.device = device
        self.reference_json_path = reference_json_path
        self.reference_data = None
        self.user_window_size = max(1, int(user_window_size))
        
        print(f"ğŸš€ Initializing Training-Consistent Tester")
        print(f"ğŸ“ Checkpoint: {checkpoint_path}")
        print(f"ğŸ“Š Dataset: {dataset_dir}")
        print(f"ğŸ¤– Model: {model_type}")
        print(f"ğŸ’» Device: {device}")
        
        # åŠ è½½æ•°æ®é›†
        self.load_dataset()
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # åˆå§‹åŒ–promptersï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        self.init_prompters()
        
        # åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å™¨
        self.init_similarity()
        
        # åŠ è½½å‚è€ƒJSONæ•°æ®(å¦‚æœæä¾›)
        if self.reference_json_path:
            self.load_reference_json()
        
    def load_reference_json(self):
        """åŠ è½½å‚è€ƒJSONæ–‡ä»¶,åŒ…å«å›ºå®šçš„æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‚ Loading reference JSON from {self.reference_json_path}")
        try:
            with open(self.reference_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.reference_data = data.get('detailed_results', [])
            
            if self.reference_data:
                print(f"âœ… Loaded {len(self.reference_data)} reference samples from JSON")
                print(f"  Sample fields: {list(self.reference_data[0].keys())}")
            else:
                print("âš ï¸  No detailed_results found in JSON file")
                self.reference_data = None
                
        except Exception as e:
            print(f"âŒ Error loading reference JSON: {e}")
            self.reference_data = None
    
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        print(f"ğŸ“‚ Loading dataset from {self.dataset_dir}")
        try:
            self.dataset = load_from_disk(self.dataset_dir)
            self.test_data = self.dataset['test']
            self.item_info = self.dataset['item_info']
            
            # åˆ›å»ºitemæ˜ å°„
            self.create_item_mappings()
            
            print(f"âœ… Dataset loaded successfully:")
            print(f"  Test samples: {len(self.test_data)}")
            print(f"  Items: {len(self.item_info)}")
            
        except Exception as e:
            print(f"âŒ Error loading dataset: {e}")
            raise
    
    def create_item_mappings(self):
        """åˆ›å»ºå•†å“æ˜ å°„"""
        self.id2title = {}
        self.all_item_ids = []
        
        # åˆ›å»ºuser_idåˆ°æ ·æœ¬ç´¢å¼•çš„æ˜ å°„
        # æ³¨æ„ï¼šåŒä¸€ user_id å¯èƒ½åœ¨ test split ä¸­å‡ºç°å¤šæ¬¡ï¼Œè¿™é‡ŒåŒæ—¶ç»´æŠ¤ï¼š
        # - user_id_to_sampleï¼šé¦–æ¬¡å‡ºç°çš„æ ·æœ¬ç´¢å¼•ï¼ˆå‘åå…¼å®¹ï¼Œä¸”é¿å…è¢«è¦†ç›–ï¼‰
        # - user_id_to_samplesï¼šè¯¥ user_id å¯¹åº”çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        self.user_id_to_sample = {}
        self.user_id_to_samples = {}
        
        for item in self.item_info:
            item_id = item['item_id']
            title = item.get('title', f'Unknown Item {item_id}')
            
            self.id2title[item_id] = title
            
            if item_id != 0:  # æ’é™¤padding token
                self.all_item_ids.append(item_id)
        
        # åˆ›å»ºuser_idç´¢å¼•
        for idx, sample in enumerate(self.test_data):
            user_id = sample.get('user_id', None)
            if user_id:
                # ä»…å½“é¦–æ¬¡é‡åˆ°è¯¥ user_id æ—¶è®°å½•ï¼Œé¿å…è¢«åç»­ç›¸åŒ user è¦†ç›–
                if user_id not in self.user_id_to_sample:
                    self.user_id_to_sample[user_id] = idx
                # è®°å½•æ‰€æœ‰ä½ç½®ï¼Œä¾¿äºè°ƒè¯•æˆ–éœ€è¦ç²¾ç¡®å®šä½æ—¶ä½¿ç”¨
                self.user_id_to_samples.setdefault(user_id, []).append(idx)
        
        print(f"ğŸ“‹ Created mappings for {len(self.id2title)} items (including {len(self.all_item_ids)} non-pad items)")
        print(f"ğŸ“‹ Created user_id mappings for {len(self.user_id_to_sample)} distinct users")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ”„ Loading model from {self.checkpoint_path}")
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„ç±»
            if self.model_type == 'qwen':
                model_name = model_names["Qwen2.5-3B-Instruct"]
                from models.qwen_models import (Qwen2RRecCasualLM as ModelClass,
                                               Qwen2RRecConfig as ConfigClass)
            elif self.model_type == 'gemma':
                model_name = model_names["Gemma-2-2b-it"]
                from models.gemma_models import (Gemma2RRecCasualLM as ModelClass,
                                                Gemma2RRecConfig as ConfigClass)
            else:
                raise NotImplementedError(f"Model type {self.model_type} not supported")
            
            # åŠ è½½tokenizer
            self.tokenizer = get_tokenizer(model_name)
            
            # åŠ è½½é…ç½®
            config = ConfigClass.from_pretrained(model_name)
            config.use_cache = False
            config.pad_token_id = self.tokenizer.pad_token_id
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print(f"ğŸ“¥ Loading base model: {model_name}")
            self.model = ModelClass.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map={"": self.device},
                config=config
            )
            
            # åŠ è½½LoRAæƒé‡
            print(f"ğŸ”§ Loading LoRA weights from {self.checkpoint_path}")
            self.model = PeftModel.from_pretrained(self.model, self.checkpoint_path)
            
            # åˆå¹¶LoRAæƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦
            print("ğŸ”„ Merging LoRA weights...")
            self.model = self.model.merge_and_unload()
            
            self.model.eval()
            print("âœ… Model loaded successfully")
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def init_prompters(self):
        """åˆå§‹åŒ–promptersï¼Œä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
        print("ğŸ”„ Initializing prompters...")
        
        # ä»æ•°æ®é›†ç›®å½•æå–category
        dataset_name = self.dataset_dir.split('/')[-1]
        if 'Musical_Instruments' in dataset_name:
            category = 'Musical_Instruments'
        elif 'Video_Games' in dataset_name:
            category = 'Video_Games'
        elif 'CDs_and_Vinyl' in dataset_name:
            category = 'CDs_and_Vinyl'
        else:
            # é»˜è®¤ä½¿ç”¨Musical_Instruments
            category = 'Musical_Instruments'
            print(f"âš ï¸  Could not determine category from {dataset_name}, using default: {category}")
        
        # è®¾ç½®ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å‚æ•°ï¼ˆå¯é…ç½®çš„ user window sizeï¼‰
        window_size = self.user_window_size
        emb_token = '<answer>'
        emb_end_token = '</answer>'
        
        # åˆ›å»ºå®Œæ•´çš„æ•°æ®é›†ç»“æ„ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„full_datasetï¼‰
        full_dataset = {
            'test': self.test_data,
            'item_info': self.item_info
        }
        
        # åˆå§‹åŒ–UserGenPrompterï¼ˆç”¨äºç”Ÿæˆuser reasoningçš„promptï¼‰
        self.user_gen_prompter = UserGenPrompter(
            dset=full_dataset,
            tokenizer=self.tokenizer,
            category=category,
            window_size=window_size,
            emb_token=emb_token,
            emb_end_token=emb_end_token
        )
        
        # åˆå§‹åŒ–UserPrompterï¼ˆç”¨äºå°†reasoningè½¬æ¢ä¸ºæœ€ç»ˆè¾“å…¥ï¼‰
        self.user_prompter = UserPrompter(
            dset=full_dataset,
            tokenizer=self.tokenizer,
            category=category,
            window_size=window_size,
            input_ids_max_length=2048,
            emb_token=emb_token,
            emb_end_token=emb_end_token
        )
        
        print("âœ… Prompters initialized")
    
    def init_similarity(self):
        """åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å™¨"""
        # åˆ›å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç›¸ä¼¼åº¦é…ç½®
        class SimilarityConfig:
            similarity_type = "dot"
            similarity_temperature = 0.02
            similarity_normalization = True
        
        config = SimilarityConfig()
        self.similarity = Similarity(config)
        print("âœ… Similarity calculator initialized")
    
    def generate_item_embeddings(self):
        """ç”Ÿæˆæ‰€æœ‰å•†å“çš„embeddingsï¼Œä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
        print("ğŸ”„ Generating item embeddings...")
        
        self.item_embeddings = {}
        batch_size = 32
        
        # å‡†å¤‡æ‰€æœ‰å•†å“çš„è¾“å…¥ï¼Œä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ ¼å¼
        all_items = []
        for item in self.item_info:
            if item['item_id'] != 0:  # æ’é™¤padding
                # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„item promptæ ¼å¼
                title = item.get('title', f"Unknown Item {item['item_id']}")
                description = item.get('description', '')
                if isinstance(description, list):
                    description = ' '.join(description[::-1]) if description else ''
                
                # æ„å»ºitem info stringï¼ˆä¸prompterä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
                average_rating = item.get('average_rating', 0.0)
                num_buyers = item.get('rating_number', 0)
                
                item_info_str = (f"Title: {title}\n"
                               f"User Rating: {average_rating}\n"
                               f"Number of Buyers: {num_buyers}\n"
                               f"Description: {description}")
                
                # è·å–categoryï¼ˆä¸init_promptersä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
                dataset_name = self.dataset_dir.split('/')[-1]
                if 'Musical_Instruments' in dataset_name:
                    category = 'Musical_Instruments'
                elif 'Video_Games' in dataset_name:
                    category = 'Video_Games'
                elif 'CDs_and_Vinyl' in dataset_name:
                    category = 'CDs_and_Vinyl'
                else:
                    category = 'Musical_Instruments'
                
                prompts = obtain_prompts(category)
                item_prompt = prompts['item_prompt'].format(
                    emb_token='<answer>',
                    emb_end_token='</answer>'
                )
                
                # æ„å»ºå®Œæ•´çš„item prompt
                full_prompt = item_prompt + '\n\n' + item_info_str
                
                # if "Milisten 2PCS Humbucker Pickup Ring Frame Mounting Ring For Electric Guitars GB301 Golden" in full_prompt:
                #     print(full_prompt)
                
                # ä½¿ç”¨chat template
                conversation = [
                    {"role": "user", "content": full_prompt},
                    {"role": "assistant", "content": "<answer>"}
                ]
                
                item_text = self.tokenizer.apply_chat_template(
                    conversation, 
                    tokenize=False, 
                    add_generation_prompt=False
                )
                
                all_items.append((item['item_id'], item_text))
        
        # æ‰¹é‡å¤„ç†
        with torch.no_grad():
            for i in tqdm(range(0, len(all_items), batch_size), desc="Generating item embeddings"):
                batch_items = all_items[i:i+batch_size]
                item_ids = [item[0] for item in batch_items]
                item_texts = [item[1] for item in batch_items]
                
                # Tokenize
                inputs = self.tokenizer(
                    item_texts,
                    padding=True,
                    truncation=True,
                    max_length=768,  # ä¸è®­ç»ƒæ—¶çš„item_input_max_lengthä¸€è‡´
                    return_tensors="pt"
                ).to(self.device)
                
                # è·å–embeddings
                outputs = self.model(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    return_causal_output=False,
                    return_with_last_hidden_states=True
                )
                
                # ä¿å­˜embeddings
                for j, item_id in enumerate(item_ids):
                    self.item_embeddings[item_id] = outputs[j].cpu()
        
        print(f"âœ… Generated embeddings for {len(self.item_embeddings)} items")
    
    def generate_user_reasoning_training_style(self, sample: Dict) -> str:
        """ä½¿ç”¨è®­ç»ƒæ—¶çš„æ–¹å¼ç”Ÿæˆuser reasoning"""
        # é‡å‘½ååˆ—ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„æ ¼å¼
        sample_copy = dict(sample)
        sample_copy['seq_input_ids'] = sample_copy['history_item_id']
        sample_copy['seq_labels'] = sample_copy['item_id']
        
        # ä½¿ç”¨UserGenPrompterç”Ÿæˆprompt
        chat_example = self.user_gen_prompter.to_chat_example(sample_copy)
        # ä¿å­˜ç”Ÿæˆçš„promptå†…å®¹ï¼Œä¾¿äºåç»­å­˜å‚¨
        self.last_user_reasoning_prompt = chat_example.get('prompt', None)
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼
        tensor_example = self.user_gen_prompter.totensor(
            chat_example, 
            max_length=2048
        )
        
        # å‡†å¤‡è¾“å…¥
        input_ids = torch.tensor(tensor_example['input_ids']).unsqueeze(0).to(self.device)
        attention_mask = torch.tensor(tensor_example['attention_mask']).unsqueeze(0).to(self.device)
        
        # ç”Ÿæˆreasoning
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=300,  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
                temperature=1.5,
                top_k=200,
                top_p=1.0,
                do_sample=True,
                # temperature=0.0,
                # do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                stop_strings=['<answer>'],  # ä¸è®­ç»ƒæ—¶çš„stop tokenä¸€è‡´
                tokenizer=self.tokenizer,  # æ·»åŠ tokenizerå‚æ•°
            )
        
        # è§£ç ç”Ÿæˆçš„reasoning
        generated_text = self.tokenizer.decode(
            outputs[0][input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        # ç¡®ä¿ä»¥<answer>ç»“å°¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        if not generated_text.endswith('<answer>'):
            generated_text += '<answer>'
        
        return generated_text
    
    def get_user_embedding_training_style(self, sample: Dict, reasoning: str) -> torch.Tensor:
        """ä½¿ç”¨è®­ç»ƒæ—¶çš„æ–¹å¼ä»reasoningæå–user embedding"""
        # åˆ›å»ºåŒ…å«reasoningçš„æ ·æœ¬
        sample_with_reasoning = dict(sample)
        sample_with_reasoning['seq_input_ids'] = sample_with_reasoning['history_item_id']
        sample_with_reasoning['seq_labels'] = sample_with_reasoning['item_id']
        sample_with_reasoning['profile'] = reasoning
        
        # ä½¿ç”¨UserPrompterè½¬æ¢ï¼ˆä¸è®­ç»ƒæ—¶çš„evaluateæ–¹æ³•ä¸€è‡´ï¼‰
        chat_example = self.user_prompter.to_chat_example(sample_with_reasoning)
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼
        tensor_example = self.user_prompter.totensor(
            chat_example,
            max_length=2048,
            continue_final_message=True
        )
        
        # å‡†å¤‡è¾“å…¥
        input_ids = torch.tensor(tensor_example['input_ids']).unsqueeze(0).to(self.device)
        attention_mask = torch.tensor(tensor_example['attention_mask']).unsqueeze(0).to(self.device)
        
        # è·å–æœ€åä¸€ä¸ªtokençš„embeddingï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        with torch.no_grad():
            user_embedding = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_causal_output=False,
                return_with_last_hidden_states=True
            )
        
        return user_embedding.squeeze(0)  # ç§»é™¤batchç»´åº¦
    
    def compute_similarity_training_style(self, user_embedding: torch.Tensor, candidate_items: List[int]) -> torch.Tensor:
        """ä½¿ç”¨è®­ç»ƒæ—¶çš„æ–¹å¼è®¡ç®—ç›¸ä¼¼åº¦"""
        # è·å–å€™é€‰å•†å“çš„embeddings
        candidate_embeddings = []
        for item_id in candidate_items:
            if item_id in self.item_embeddings:
                candidate_embeddings.append(self.item_embeddings[item_id])
            else:
                # å¦‚æœæ²¡æœ‰embeddingï¼Œä½¿ç”¨é›¶å‘é‡
                candidate_embeddings.append(torch.zeros_like(user_embedding))
        
        candidate_embeddings = torch.stack(candidate_embeddings).to(self.device)
        user_embedding = user_embedding.to(self.device)
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼
        similarity_scores = self.similarity(user_embedding.unsqueeze(0), candidate_embeddings)
        
        return similarity_scores.squeeze(0)  # ç§»é™¤batchç»´åº¦
    
    def calculate_metrics_like_training(self, predicted_ranking: List[int], target_id: int) -> Dict[str, float]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´
        """
        metrics = {}
        
        # æ‰¾åˆ°ç›®æ ‡å•†å“åœ¨é¢„æµ‹æ’åºä¸­çš„ä½ç½®ï¼ˆ0-indexedï¼‰
        try:
            target_position = predicted_ranking.index(target_id)
        except ValueError:
            # ç›®æ ‡å•†å“ä¸åœ¨é¢„æµ‹ä¸­ï¼Œè®¾ä¸ºåˆ—è¡¨é•¿åº¦ï¼ˆè¡¨ç¤ºæ’åœ¨æœ€åï¼‰
            target_position = len(predicted_ranking)
        
        # è®¡ç®—Hit Rate @ K
        for k in [5, 10, 20]:
            metrics[f'hit_rate@{k}'] = 1.0 if target_position < k else 0.0
        
        # è®¡ç®—DCG @ Kï¼ˆä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼‰
        for k in [5, 10, 20]:
            if target_position < k:
                # ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼šposition 0å¯¹åº”log2(2), position 1å¯¹åº”log2(3)...
                dcg_value = 1.0 / np.log2(target_position + 2)
                metrics[f'ndcg@{k}'] = dcg_value
            else:
                metrics[f'ndcg@{k}'] = 0.0
        
        return metrics
    
    def evaluate_training_consistent(self, num_samples: int = 622, save_results: bool = True) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€è‡´æ€§è¯„ä¼°å‡½æ•°
        
        Args:
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
            save_results: æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœ
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        # å¦‚æœæä¾›äº†å‚è€ƒJSON,ä½¿ç”¨å›ºå®šæ•°æ®è¯„ä¼°
        if self.reference_data is not None:
            return self.evaluate_with_reference_data(save_results=save_results)
        
        print(f"\nğŸ¯ Starting Training-Consistent evaluation on first {num_samples} test samples")
        print("=" * 60)
        
        # ç”Ÿæˆå•†å“embeddings
        self.generate_item_embeddings()
        
        # åˆå§‹åŒ–ç´¯ç§¯æŒ‡æ ‡
        accumulated_metrics = {
            'hit_rate@5': 0.0, 'hit_rate@10': 0.0, 'hit_rate@20': 0.0,
            'ndcg@5': 0.0, 'ndcg@10': 0.0, 'ndcg@20': 0.0
        }
        
        valid_samples = 0
        detailed_results = []
        
        for i in tqdm(range(min(num_samples, len(self.test_data))), desc="Evaluating"):
            sample = self.test_data[i]
            
            # è·å–å†å²å’Œç›®æ ‡
            history_ids = sample['history_item_id']
            target_id = sample['item_id']
            
            # è·³è¿‡padding token
            if target_id == 0:
                continue
                
            # è¿‡æ»¤å†å²ä¸­çš„padding token
            history_ids = [id for id in history_ids if id != 0]
            history_titles = [self.id2title.get(id, f"Unknown_{id}") for id in history_ids]
            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")
            
            # æ„å»ºå¯ç”¨å•†å“é›†åˆï¼šæ’é™¤å†å²äº¤äº’å•†å“
            available_items = [item_id for item_id in self.all_item_ids 
                             if item_id not in history_ids]
            
            # ç¡®ä¿ç›®æ ‡å•†å“åœ¨å¯ç”¨å•†å“ä¸­
            if target_id not in available_items:
                print(f"Warning: Target item {target_id} is in history for sample {i}")
                continue
            
            # R1é‡‡æ ·é€»è¾‘ï¼šéšæœºé€‰æ‹©20ä¸ªå€™é€‰å•†å“
            sample_size = min(20, len(available_items))
            
            # ç¡®ä¿ç›®æ ‡å•†å“åœ¨å€™é€‰ä¸­
            other_items = [item for item in available_items if item != target_id]
            if len(other_items) >= sample_size - 1:
                sampled_others = random.sample(other_items, sample_size - 1)
                candidate_items = [target_id] + sampled_others
            else:
                candidate_items = available_items
            
            random.shuffle(candidate_items)
            
            # ç”Ÿæˆç”¨æˆ·æ¨ç†ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            user_reasoning = self.generate_user_reasoning_training_style(sample)
            
            # ä»æ¨ç†ä¸­æå–ç”¨æˆ·embeddingï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            user_embedding = self.get_user_embedding_training_style(sample, user_reasoning)
            
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            similarity_scores = self.compute_similarity_training_style(user_embedding, candidate_items)
            
            # æ’åºè·å¾—é¢„æµ‹ç»“æœ
            _, sorted_indices = torch.sort(similarity_scores, descending=True)
            predicted_ranking = [candidate_items[idx] for idx in sorted_indices.cpu().tolist()]
            
            # è®¡ç®—æŒ‡æ ‡
            sample_metrics = self.calculate_metrics_like_training(predicted_ranking, target_id)
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in accumulated_metrics:
                accumulated_metrics[key] += sample_metrics[key]
            
            valid_samples += 1
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            if save_results:
                target_rank = predicted_ranking.index(target_id) + 1 if target_id in predicted_ranking else len(predicted_ranking) + 1
                detailed_results.append({
                    'sample_id': i,
                    'user_id': sample.get('user_id', 'unknown'),
                    'history_titles': history_titles[-self.user_window_size:],  # ä¿å­˜æœ€åNæ¡å†å²
                    'target_title': target_title,
                    'target_id': target_id,
                    'target_rank': target_rank,
                    'total_candidates': len(candidate_items),
                    'candidate_items': candidate_items,  # ä¿å­˜å€™é€‰å•†å“åˆ—è¡¨ï¼Œæ–¹ä¾¿ä¸R1å¯¹æ¯”
                    'predicted_ranking': predicted_ranking,  # æ–°å¢å­—æ®µï¼Œé¡ºåºå³ä¸ºæ’å
                    'user_reasoning': user_reasoning,  # ä¿å­˜å®Œæ•´çš„reasoningï¼Œä¸æˆªæ–­
                    'user_reasoning_prompt': getattr(self, 'last_user_reasoning_prompt', None),  # æ–°å¢ï¼Œä¿å­˜ç”Ÿæˆreasoningæ—¶çš„prompt
                    'metrics': sample_metrics
                })

            # æ¯1ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if valid_samples % 1 == 0:
                current_metrics = {k: v / valid_samples for k, v in accumulated_metrics.items()}
                print(f"\n--- Progress: {valid_samples}/{num_samples} samples ---")
                print(f"Current NDCG@10: {current_metrics['ndcg@10']:.6f}")
                print(f"Current Hit Rate@10: {current_metrics['hit_rate@10']:.6f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if valid_samples > 0:
            for key in accumulated_metrics:
                accumulated_metrics[key] = accumulated_metrics[key] / valid_samples
        
        accumulated_metrics['total_samples'] = valid_samples
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\n{'='*60}")
        print(f"FINAL TRAINING-CONSISTENT EVALUATION RESULTS ({valid_samples} samples)")
        print(f"{'='*60}")
        print(f"hit_rate@5:  {accumulated_metrics['hit_rate@5']:.6f}")
        print(f"hit_rate@10: {accumulated_metrics['hit_rate@10']:.6f}")
        print(f"hit_rate@20: {accumulated_metrics['hit_rate@20']:.6f}")
        print(f"ndcg@5:      {accumulated_metrics['ndcg@5']:.6f}")
        print(f"ndcg@10:     {accumulated_metrics['ndcg@10']:.6f}")
        print(f"ndcg@20:     {accumulated_metrics['ndcg@20']:.6f}")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = f"training_consistent_results_{self.model_type}_{num_samples}samples_{timestamp}.json"
            
            results = {
                'metrics': accumulated_metrics,
                'detailed_results': detailed_results,
                'config': {
                    'checkpoint_path': self.checkpoint_path,
                    'dataset_dir': self.dataset_dir,
                    'model_type': self.model_type,
                    'num_samples': num_samples,
                    'evaluation_method': 'training_consistent',
                    'device': self.device,
                    'note': 'This evaluation strictly follows the training evaluation process'
                }
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ Results saved to: {results_file}")
        
        return accumulated_metrics
    
    def evaluate_with_reference_data(self, save_results: bool = True) -> Dict[str, float]:
        """
        ä½¿ç”¨å‚è€ƒJSONä¸­çš„å›ºå®šæ•°æ®è¿›è¡Œè¯„ä¼°
        
        Args:
            save_results: æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœ
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print(f"\nğŸ¯ Starting evaluation with FIXED reference data from JSON")
        print(f"ğŸ“Š Total reference samples: {len(self.reference_data)}")
        print("=" * 60)
        
        # ç”Ÿæˆå•†å“embeddings
        self.generate_item_embeddings()
        
        # åˆå§‹åŒ–ç´¯ç§¯æŒ‡æ ‡
        accumulated_metrics = {
            'hit_rate@5': 0.0, 'hit_rate@10': 0.0, 'hit_rate@20': 0.0,
            'ndcg@5': 0.0, 'ndcg@10': 0.0, 'ndcg@20': 0.0
        }
        
        valid_samples = 0
        detailed_results = []
        
        for ref_idx, ref_sample in enumerate(tqdm(self.reference_data, desc="Evaluating with reference data")):
            # ä»å‚è€ƒæ•°æ®ä¸­è·å–ä¿¡æ¯
            user_id = ref_sample.get('user_id')
            candidate_ids = ref_sample.get('candidate_ids')
            sample_index = ref_sample.get('sample_index')
            
            if not user_id or not candidate_ids:
                print(f"âš ï¸  Skipping reference sample {ref_idx}: missing user_id or candidate_ids")
                continue
            
            # é€‰æ‹©æ ·æœ¬ä¼˜å…ˆçº§ï¼šä¼˜å…ˆä¿¡ä»» JSON ä¸­çš„ sample_indexï¼Œå…¶æ¬¡æ ¹æ® user_id æŸ¥æ‰¾
            sample_idx = None
            # 1) ä¼˜å…ˆä½¿ç”¨ JSON é‡Œè®°å½•çš„ sample_indexï¼ˆè¿™ä¸ test_same_with_test çš„è®°å½•ä¸€ä¸€å¯¹åº”ï¼‰
            if isinstance(sample_index, int) and 0 <= sample_index < len(self.test_data):
                sample_idx = sample_index
            else:
                # 2) å›é€€åˆ° user_id æ˜ å°„
                if user_id in self.user_id_to_sample:
                    sample_idx = self.user_id_to_sample[user_id]
                else:
                    print(f"âš ï¸  User {user_id} not found in test dataset; skipping ref #{ref_idx}")
                    continue

            sample = self.test_data[sample_idx]

            # ä¸€è‡´æ€§æ£€æŸ¥ï¼šè‹¥ JSON ç»™äº† sample_index ä¸” user_id ä¸ä¸€è‡´ï¼Œæ‰“å°æ›´æ˜ç¡®çš„æç¤º
            if isinstance(sample_index, int) and 0 <= sample_index < len(self.test_data):
                sample_user_id = sample.get('user_id', None)
                if str(sample_user_id) != str(user_id):
                    # åœ¨æ•°æ®é›†ä¸­æŸ¥æ‰¾è¯¥ user_id çš„æ‰€æœ‰ä½ç½®ï¼Œå¸®åŠ©è¯Šæ–­
                    candidates = self.user_id_to_samples.get(user_id, [])
                    print(
                        f"âš ï¸  Mismatch at ref #{ref_idx}: JSON(sample_index={sample_index}, user_id={user_id}) "
                        f"but dataset[{sample_idx}].user_id={sample_user_id}. Possible duplicate user entries. "
                        f"All indices for this user: {candidates[:5]}{'...' if len(candidates) > 5 else ''}"
                    )
            
            # è·å–å†å²å’Œç›®æ ‡
            history_ids = sample['history_item_id']
            target_id = sample['item_id']
            
            # è·³è¿‡padding token
            if target_id == 0:
                print(f"âš ï¸  Skipping sample {sample_idx}: target_id is 0")
                continue
            
            # è¿‡æ»¤å†å²ä¸­çš„padding token
            history_ids = [id for id in history_ids if id != 0]
            history_titles = [self.id2title.get(id, f"Unknown_{id}") for id in history_ids]
            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")
            
            # ä½¿ç”¨JSONä¸­å›ºå®šçš„å€™é€‰å•†å“åˆ—è¡¨
            candidate_items = candidate_ids.copy()
            
            # éªŒè¯ç›®æ ‡å•†å“æ˜¯å¦åœ¨å€™é€‰ä¸­
            if target_id not in candidate_items:
                print(f"âš ï¸  Warning: target_id {target_id} not in candidate_ids for sample {sample_idx}")
                # å¯ä»¥é€‰æ‹©è·³è¿‡æˆ–æ·»åŠ ç›®æ ‡å•†å“
                # candidate_items.append(target_id)
                continue
            
            # éªŒè¯å€™é€‰å•†å“çš„æœ‰æ•ˆæ€§
            valid_candidates = []
            for item_id in candidate_items:
                if item_id in self.item_embeddings:
                    valid_candidates.append(item_id)
                else:
                    print(f"âš ï¸  Warning: candidate item {item_id} not found in embeddings")
            
            if not valid_candidates:
                print(f"âš ï¸  Skipping sample {sample_idx}: no valid candidates")
                continue
            
            candidate_items = valid_candidates
            
            # ç”Ÿæˆç”¨æˆ·æ¨ç†ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            user_reasoning = self.generate_user_reasoning_training_style(sample)
            
            # ä»æ¨ç†ä¸­æå–ç”¨æˆ·embeddingï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            user_embedding = self.get_user_embedding_training_style(sample, user_reasoning)
            
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            similarity_scores = self.compute_similarity_training_style(user_embedding, candidate_items)
            
            # æ’åºè·å¾—é¢„æµ‹ç»“æœ
            _, sorted_indices = torch.sort(similarity_scores, descending=True)
            predicted_ranking = [candidate_items[idx] for idx in sorted_indices.cpu().tolist()]
            
            # è®¡ç®—æŒ‡æ ‡
            sample_metrics = self.calculate_metrics_like_training(predicted_ranking, target_id)
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in accumulated_metrics:
                accumulated_metrics[key] += sample_metrics[key]
            
            valid_samples += 1
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            if save_results:
                target_rank = predicted_ranking.index(target_id) + 1 if target_id in predicted_ranking else len(predicted_ranking) + 1
                detailed_results.append({
                    'sample_id': sample_idx,
                    'reference_index': ref_idx,
                    'user_id': user_id,
                    'history_titles': history_titles[-self.user_window_size:],
                    'target_title': target_title,
                    'target_id': target_id,
                    'target_rank': target_rank,
                    'total_candidates': len(candidate_items),
                    'candidate_items': candidate_items,
                    'predicted_ranking': predicted_ranking,
                    'similarity_scores': similarity_scores.cpu().tolist(),
                    'user_reasoning': user_reasoning,
                    'user_reasoning_prompt': getattr(self, 'last_user_reasoning_prompt', None),
                    'metrics': sample_metrics,
                    'reference_data': ref_sample  # ä¿å­˜åŸå§‹å‚è€ƒæ•°æ®ç”¨äºå¯¹æ¯”
                })
            
            # æ¯10ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if valid_samples % 10 == 0:
                current_metrics = {k: v / valid_samples for k, v in accumulated_metrics.items()}
                print(f"\n--- Progress: {valid_samples}/{len(self.reference_data)} samples ---")
                print(f"Current NDCG@10: {current_metrics['ndcg@10']:.6f}")
                print(f"Current Hit Rate@10: {current_metrics['hit_rate@10']:.6f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if valid_samples > 0:
            for key in accumulated_metrics:
                accumulated_metrics[key] /= valid_samples
        
        accumulated_metrics['total_samples'] = valid_samples
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\n{'='*60}")
        print(f"FINAL EVALUATION RESULTS WITH REFERENCE DATA ({valid_samples} samples)")
        print(f"{'='*60}")
        print(f"hit_rate@5:  {accumulated_metrics['hit_rate@5']:.6f}")
        print(f"hit_rate@10: {accumulated_metrics['hit_rate@10']:.6f}")
        print(f"hit_rate@20: {accumulated_metrics['hit_rate@20']:.6f}")
        print(f"ndcg@5:      {accumulated_metrics['ndcg@5']:.6f}")
        print(f"ndcg@10:     {accumulated_metrics['ndcg@10']:.6f}")
        print(f"ndcg@20:     {accumulated_metrics['ndcg@20']:.6f}")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = f"reference_based_results_{self.model_type}_{valid_samples}samples_{timestamp}.json"
            
            results = {
                'metrics': accumulated_metrics,
                'detailed_results': detailed_results,
                'config': {
                    'checkpoint_path': self.checkpoint_path,
                    'dataset_dir': self.dataset_dir,
                    'model_type': self.model_type,
                    'reference_json_path': self.reference_json_path,
                    'num_samples': valid_samples,
                    'evaluation_method': 'reference_based',
                    'device': self.device,
                    'note': 'This evaluation uses fixed data from reference JSON file'
                }
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ Results saved to: {results_file}")
        
        return accumulated_metrics

def main():
    parser = argparse.ArgumentParser(description="Training-consistent checkpoint testing")
    parser.add_argument("--checkpoint_path", type=str, 
                       default="/data2/home/hongdeyao/RRec/checkpoint-test",
                       help="Checkpoint path")
    parser.add_argument("--dataset_dir", type=str, 
                       default="/data/hongdeyao/Musical_Instruments_0_2022-10-2023-10",
                       help="Dataset directory")
    parser.add_argument("--num_samples", type=int, default=100,
                       help="Number of test samples (ignored if reference_json is provided)")
    parser.add_argument("--model_type", type=str, default="qwen",
                       choices=["qwen", "gemma"],
                       help="Model type")
    parser.add_argument("--device", type=str, default="cuda:0",
                       help="Device to use")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed")
    parser.add_argument("--reference_json", type=str, default=None,
                       help="Path to reference JSON file with fixed user_id and candidate_ids")
    parser.add_argument("--user_window_size", type=int, default=20,
                       help="How many most recent user history interactions to keep when building prompts")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(args.checkpoint_path):
        print(f"âŒ Checkpoint path not found: {args.checkpoint_path}")
        return
    
    if not os.path.exists(args.dataset_dir):
        print(f"âŒ Dataset directory not found: {args.dataset_dir}")
        return
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = TrainingConsistentTester(
        checkpoint_path=args.checkpoint_path,
        dataset_dir=args.dataset_dir,
        model_type=args.model_type,
        device=args.device,
        reference_json_path=args.reference_json,
        user_window_size=args.user_window_size
    )
    
    # è¿è¡Œè¯„ä¼°
    results = tester.evaluate_training_consistent(
        num_samples=args.num_samples,
        save_results=True
    )
    
    print("\nğŸ‰ Training-consistent evaluation completed successfully!")

if __name__ == "__main__":
    main()
