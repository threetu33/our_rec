import os
import json
import numpy as np
import pandas as pd
from datasets import load_from_disk
from tqdm import tqdm
from typing import List, Dict, Any, Optional
import time
import argparse
from openai import OpenAI
import random
import re

# --test_only

class APIRecommendationTester:
    VALID_TARGET_POSITIONS = {"first", "last", "random"}

    def __init__(self, 
                 api_key: str, 
                 dataset_dir: str,
                 api_base: str = "http://115.182.62.174:18888/v1",
                 model_name: str = "deepseek-reasoner",
                 seed: int = 42,
                 target_position: str = "first",
                 output_dir: str = ".",
                 test_limit: Optional[int] = None,
                 predefined_candidates_path: Optional[str] = None,
                 predefined_candidate_split: str = "test"):
        """
        åˆå§‹åŒ–APIæµ‹è¯•å™¨ï¼Œä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´çš„è¯„ä¼°æ–¹å¼
        
        Args:
            api_key: APIå¯†é’¥
            dataset_dir: æ•°æ®é›†ç›®å½•è·¯å¾„
            api_base: APIåŸºç¡€URL
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            seed: éšæœºç§å­
            target_position: æ­£ç¡®å€™é€‰åœ¨å€™é€‰åˆ—è¡¨ä¸­çš„ä½ç½®ç­–ç•¥ï¼ˆfirst/last/randomï¼‰
            output_dir: ç»“æœæ–‡ä»¶ä¿å­˜ç›®å½•
            test_limit: test-onlyæ¨¡å¼ä¸‹è¯„ä¼°çš„æœ€å¤§æ ·æœ¬æ•°é‡ï¼ˆæŒ‰æ•°æ®é¡ºåºï¼‰
        """
        self.api_key = api_key
        self.dataset_dir = dataset_dir
        self.api_base = api_base
        self.model_name = model_name
        self.seed = seed
        self.target_position_mode = target_position.lower()
        if self.target_position_mode not in self.VALID_TARGET_POSITIONS:
            raise ValueError(f"target_position must be one of {sorted(self.VALID_TARGET_POSITIONS)}, got {target_position}")
        self.output_dir = os.path.abspath(output_dir)
        os.makedirs(self.output_dir, exist_ok=True)
        self.test_limit = test_limit
        self.test_only_mode = False
        self.predefined_candidate_split = predefined_candidate_split.lower() if predefined_candidate_split else "test"
        self.predefined_candidates_path = predefined_candidates_path
        self.predefined_candidate_entries: Optional[List[Dict[str, Any]]] = None
        self.predefined_candidate_config: Dict[str, Any] = {}
        self._split_user_index_cache: Dict[str, Dict[str, List[int]]] = {}
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(base_url=api_base, api_key=api_key)
        
        # åŠ è½½æ•°æ®é›†
        self.load_dataset()

        if self.predefined_candidates_path:
            self.load_predefined_candidates(self.predefined_candidates_path)

    def prepare_rank_candidates(self, 
                                 data_split: List[Dict[str, Any]],
                                 num_users: Optional[int],
                                 split_name: str,
                                 preserve_order: bool = False) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä¸run_rrec.pyå’Œgenerate_prompts_rank.pyä¸€è‡´çš„æ–¹å¼æ„å»ºå€™é€‰é›†åˆ"""
        if self.predefined_candidate_entries and split_name == self.predefined_candidate_split:
            return self.prepare_rank_candidates_from_predefined(data_split, num_users, split_name)
        order_note = "preserving dataset order" if preserve_order else "with shuffled users"
        print(f"\nğŸ”„ Preparing rank candidates for {split_name} split using training-consistent sampler ({order_note})")

        user_samples: Dict[Any, List[Dict[str, Any]]] = {}
        for idx in range(len(data_split)):
            sample = data_split[idx]
            user_id = sample.get('user_id')
            if user_id is None:
                continue
            if user_id not in user_samples:
                user_samples[user_id] = []
            user_samples[user_id].append({'index': idx, 'sample': sample})

        available_users = list(user_samples.keys())
        if not preserve_order:
            random.shuffle(available_users)

        if not available_users:
            print(f"âŒ Error: No valid users found in {split_name} split")
            return []

        requested_users = num_users if num_users is not None else len(available_users)
        if requested_users > len(available_users):
            print(f"âš ï¸ Warning: Requested {requested_users} users but only {len(available_users)} available in {split_name}, adjusting")
            requested_users = len(available_users)

        prepared_samples: List[Dict[str, Any]] = []

        for user_id in available_users:
            if len(prepared_samples) >= requested_users:
                break

            entries = user_samples[user_id]
            selected_entry: Optional[Dict[str, Any]] = None
            for entry in entries:
                target_id = entry['sample'].get('item_id', 0)
                if target_id and target_id in self.id2title:
                    selected_entry = entry
                    break

            if selected_entry is None:
                print(f"Warning: No valid sample found for user {user_id}, skipping")
                continue

            sample_idx = selected_entry['index']
            sample = selected_entry['sample']

            history_ids = [hid for hid in sample['history_item_id'] if hid != 0]
            # if len(history_ids) < 5:
            #     print(f"Warning: User {user_id} has only {len(history_ids)} history items, skipping")
            #     continue

            target_id = sample['item_id']
            if target_id == 0:
                print(f"Warning: Sample {sample_idx} for user {user_id} has padding target, skipping")
                continue

            available_items = [item_id for item_id in self.all_item_ids if item_id not in history_ids]
            if target_id not in available_items:
                available_items.append(target_id)

            wrong_pool = [item_id for item_id in available_items if item_id != target_id]
            if len(wrong_pool) < 19:
                print(f"Warning: User {user_id} only has {len(wrong_pool)} wrong candidates available, skipping")
                continue

            wrong_candidates = random.sample(wrong_pool, 19)
            candidate_items = wrong_candidates.copy()

            if self.target_position_mode == "first":
                candidate_items.insert(0, target_id)
            elif self.target_position_mode == "last":
                candidate_items.append(target_id)
            else:
                insert_idx = random.randint(0, len(candidate_items))
                candidate_items.insert(insert_idx, target_id)

            history_titles = [self.id2title.get(hid, f"Unknown_{hid}") for hid in history_ids]
            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")

            prepared_samples.append({
                'sample_index': sample_idx,
                'sample': sample,
                'user_id': user_id,
                'history_ids': history_ids,
                'history_titles': history_titles,
                'target_id': target_id,
                'target_title': target_title,
                'candidate_items': candidate_items,
                'target_candidate_position': candidate_items.index(target_id) + 1
            })

            print(f"âœ… Prepared candidate set for user {user_id} (sample {sample_idx}) in {split_name}")

        if len(prepared_samples) < requested_users:
            print(f"âš ï¸ Warning: Only prepared {len(prepared_samples)} user samples (requested {requested_users}) for {split_name}")

        return prepared_samples

    def prepare_rank_candidates_from_predefined(self,
                                                data_split: List[Dict[str, Any]],
                                                num_users: Optional[int],
                                                split_name: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é¢„å®šä¹‰çš„JSONå€™é€‰é›†åˆæ¥æ„å»ºè¯„ä¼°æ ·æœ¬"""
        print(f"\nğŸ—‚ï¸  Preparing rank candidates for {split_name} split using predefined JSON entries (preserving file order)")

        if not self.predefined_candidate_entries:
            print("âš ï¸  Warning: No predefined entries available; returning empty list")
            return []

        requested_users = num_users if num_users is not None else len(self.predefined_candidate_entries)
        if requested_users > len(self.predefined_candidate_entries):
            print(f"âš ï¸  Warning: Requested {requested_users} entries but only {len(self.predefined_candidate_entries)} available; adjusting")
            requested_users = len(self.predefined_candidate_entries)

        user_index_map = self._build_split_user_index(data_split, split_name)

        prepared_samples: List[Dict[str, Any]] = []

        for entry_idx, entry in enumerate(self.predefined_candidate_entries[:requested_users]):
            sample_index_raw = entry.get('sample_index')
            user_id = entry.get('user_id')
            target_id_raw = entry.get('target_item_id')
            candidate_ids = entry.get('candidate_ids') or []

            if not candidate_ids:
                print(f"âš ï¸  Warning: Entry #{entry_idx} missing candidate IDs, skipping")
                continue

            sample_index: Optional[int] = None
            if sample_index_raw is not None:
                try:
                    sample_index = int(sample_index_raw)
                except (TypeError, ValueError):
                    sample_index = None

            target_id: Optional[int] = None
            if target_id_raw is not None:
                try:
                    target_id = int(target_id_raw)
                except (TypeError, ValueError):
                    target_id = None

            sample = None
            resolved_sample_index: Optional[int] = None

            if sample_index is not None and 0 <= sample_index < len(data_split):
                candidate_sample = data_split[sample_index]
                if user_id is None or candidate_sample.get('user_id') == user_id:
                    sample = candidate_sample
                    resolved_sample_index = sample_index

            if sample is None and user_id is not None:
                candidate_indices = user_index_map.get(user_id, [])
                for idx in candidate_indices:
                    candidate_sample = data_split[idx]
                    if target_id is None or candidate_sample.get('item_id') == target_id:
                        sample = candidate_sample
                        resolved_sample_index = idx
                        break
                if sample is None and candidate_indices:
                    sample = data_split[candidate_indices[0]]
                    resolved_sample_index = candidate_indices[0]

            if sample is None:
                print(f"âš ï¸  Warning: Unable to locate dataset sample for entry #{entry_idx} (user_id={user_id}, sample_index={sample_index}), skipping")
                continue

            history_ids = [hid for hid in sample['history_item_id'] if hid != 0]
            history_titles = [self.id2title.get(hid, f"Unknown_{hid}") for hid in history_ids]
            dataset_target_id = sample.get('item_id', 0)

            if target_id in (None, 0):
                target_id = dataset_target_id

            if target_id == 0:
                print(f"âš ï¸  Warning: Entry #{entry_idx} has invalid target item (0), skipping")
                continue

            candidate_items = list(dict.fromkeys(candidate_ids))  # å»é‡ä¸”ä¿æŒé¡ºåº
            if target_id not in candidate_items:
                candidate_items.append(target_id)
                print(f"â„¹ï¸  Info: Target item {target_id} not in candidate list for entry #{entry_idx}; appended to the end")

            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")

            try:
                target_candidate_position = candidate_items.index(target_id) + 1
            except ValueError:
                target_candidate_position = None

            prepared_samples.append({
                'sample_index': resolved_sample_index if resolved_sample_index is not None else -1,
                'sample': sample,
                'user_id': user_id or sample.get('user_id'),
                'history_ids': history_ids,
                'history_titles': history_titles,
                'target_id': target_id,
                'target_title': target_title,
                'candidate_items': candidate_items,
                'target_candidate_position': target_candidate_position
            })

            print(f"âœ… Prepared JSON candidate set for user {user_id or sample.get('user_id')} (entry #{entry_idx}, dataset index {resolved_sample_index})")

        if len(prepared_samples) < requested_users:
            print(f"âš ï¸  Warning: Only prepared {len(prepared_samples)} JSON-defined samples (requested {requested_users}) for {split_name}")

        return prepared_samples
        
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†ï¼Œä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´"""
        print(f"Loading dataset from {self.dataset_dir}")
        try:
            self.dataset = load_from_disk(self.dataset_dir)
            self.test_data = self.dataset['test']
            self.valid_data = self.dataset['valid']
            self.train_data = self.dataset['train']
            self.item_info = self.dataset['item_info']
            
            # åˆ›å»ºitemæ˜ å°„ï¼Œä¸è®­ç»ƒè„šæœ¬ä¸€è‡´
            self.create_item_mappings()
            
            print(f"Dataset loaded successfully:")
            print(f"  Train samples: {len(self.train_data)}")
            print(f"  Valid samples: {len(self.valid_data)}")
            print(f"  Test samples: {len(self.test_data)}")
            print(f"  Items: {len(self.item_info)}")
            
        except Exception as e:
            print(f"Error loading dataset: {e}")
            raise
    
    def create_item_mappings(self):
        """åˆ›å»ºå•†å“æ˜ å°„ï¼Œä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´"""
        self.id2title = {}
        self.id2asin = {}
        self.asin2title = {}
        self.all_item_ids = []
        
        for item in self.item_info:
            item_id = item['item_id']
            title = item.get('title', f'Unknown Item {item_id}')
            asin = item.get('parent_asin', f'unknown_asin_{item_id}')
            
            self.id2title[item_id] = title
            self.id2asin[item_id] = asin
            self.asin2title[asin] = title
            
            if item_id != 0:  # æ’é™¤padding tokenï¼Œä¸è®­ç»ƒè„šæœ¬ä¸€è‡´
                self.all_item_ids.append(item_id)
        
        print(f"Created mappings for {len(self.id2title)} items (including {len(self.all_item_ids)} non-pad items)")

    def load_predefined_candidates(self, path: str) -> None:
        """åŠ è½½é¢„å®šä¹‰çš„å€™é€‰é›†åˆï¼Œç”¨äºJSONæ¨¡å¼çš„æ•°æ®æŠ½å–"""
        resolved_path = os.path.abspath(path)
        if not os.path.exists(resolved_path):
            print(f"âš ï¸  Warning: Predefined candidate file not found at {resolved_path}. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        try:
            with open(resolved_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as exc:
            print(f"âš ï¸  Warning: Failed to load predefined candidate file: {exc}. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        entries = []
        if isinstance(data, dict):
            self.predefined_candidate_config = data.get('config', {})
            entries = data.get('detailed_results') or []
        elif isinstance(data, list):
            entries = data
        else:
            entries = []

        if not isinstance(entries, list) or not entries:
            print("âš ï¸  Warning: Predefined candidate file does not contain usable entries. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        # è§„èŒƒåŒ–å­—æ®µï¼Œç¡®ä¿å€™é€‰IDä¸ºæ•´æ•°
        normalized_entries: List[Dict[str, Any]] = []
        for entry in entries:
            if not isinstance(entry, dict):
                continue
            original_candidate_ids = entry.get('candidate_ids') or []
            try:
                candidate_ids: List[int] = []
                for item in original_candidate_ids:
                    value = int(item)
                    if value != 0:
                        candidate_ids.append(value)
            except Exception:
                # å¦‚æœè½¬æ¢å¤±è´¥åˆ™è·³è¿‡è¯¥æ¡ç›®
                continue
            normalized_entries.append({
                **entry,
                'candidate_ids': candidate_ids
            })

        if not normalized_entries:
            print("âš ï¸  Warning: No valid entries found in predefined candidate file. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        self.predefined_candidate_entries = normalized_entries
        total_entries = len(self.predefined_candidate_entries)
        print(f"ğŸ“„ Loaded {total_entries} predefined candidate entries from {resolved_path}")

        # æ ¡éªŒæ•°æ®é›†è·¯å¾„æ˜¯å¦åŒ¹é…
        expected_dataset = self.predefined_candidate_config.get('dataset_path')
        if expected_dataset and os.path.normpath(expected_dataset) != os.path.normpath(self.dataset_dir):
            print(f"âš ï¸  Warning: Candidate file dataset ({expected_dataset}) does not match current dataset ({self.dataset_dir})")

    def _build_split_user_index(self, data_split: List[Dict[str, Any]], split_name: str) -> Dict[str, List[int]]:
        """ç¼“å­˜æŒ‡å®šåˆ’åˆ†ä¸‹user_idåˆ°æ ·æœ¬ç´¢å¼•çš„æ˜ å°„ï¼ŒåŠ é€ŸæŸ¥æ‰¾"""
        cache = self._split_user_index_cache.get(split_name)
        if cache is not None:
            return cache

        mapping: Dict[str, List[int]] = {}
        for idx in range(len(data_split)):
            sample = data_split[idx]
            user_id = sample.get('user_id')
            if user_id is None:
                continue
            mapping.setdefault(user_id, []).append(idx)

        self._split_user_index_cache[split_name] = mapping
        return mapping
    
    def call_api(self, prompt: str, max_retries: int = 5) -> tuple:
        """
        è°ƒç”¨APIï¼Œä½¿ç”¨æä¾›çš„æ¥å£æ–¹å¼
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            tuple: (APIå“åº”å†…å®¹, æ€è€ƒè¿‡ç¨‹, åŸå§‹å“åº”å­—å…¸)
        """
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful recommendation system assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=3000
                )
                
                # è·å–å“åº”å†…å®¹å’Œæ€è€ƒè¿‡ç¨‹
                content = response.choices[0].message.content or ""
                reasoning = getattr(response.choices[0].message, 'reasoning', None) or ""
                
                # è½¬æ¢ä¸ºå­—å…¸ç”¨äºè°ƒè¯•
                raw_response = response.model_dump() if hasattr(response, 'model_dump') else str(response)
                
                # å¦‚æœcontentä¸ºç©ºï¼Œæ‰“å°åŸå§‹å“åº”å¹¶é‡è¯•
                if not content:
                    print(f"âš ï¸ WARNING: Empty response content received (attempt {attempt + 1}/{max_retries})!")
                    print(f"Raw response: {json.dumps(raw_response, indent=2, ensure_ascii=False) if isinstance(raw_response, dict) else raw_response}")
                    
                    if attempt < max_retries - 1:
                        print(f"ğŸ”„ Retrying due to empty content...")
                        time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue  # é‡è¯•
                    else:
                        print(f"âŒ All {max_retries} attempts returned empty content!")
                        return "", reasoning, raw_response
                
                # contentä¸ä¸ºç©ºï¼Œè¿”å›æˆåŠŸç»“æœ
                return content, reasoning, raw_response
                
            except Exception as e:
                print(f"API call failed (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    return "", "", {"error": str(e)}
    
    def create_full_ranking_prompt(self, 
                                 history_titles: List[str], 
                                 all_available_items: List[int],
                                 category: str) -> str:
        """
        åˆ›å»ºå…¨åº“å•†å“æ’åºçš„æç¤ºï¼Œä¸è®­ç»ƒè„šæœ¬çš„å…¨åº“è¯„ä¼°å®Œå…¨ä¸€è‡´
        
        Args:
            history_titles: ç”¨æˆ·å†å²å•†å“æ ‡é¢˜
            all_available_items: æ‰€æœ‰å¯ç”¨å•†å“IDï¼ˆæ’é™¤å†å²å•†å“ï¼‰
            category: å•†å“ç±»åˆ«
        """
        # å¤„ç†å†å²åºåˆ—ï¼Œä¸è®­ç»ƒæ—¶çš„window_size=20ä¿æŒä¸€è‡´
        history_display = history_titles[-20:] if len(history_titles) > 20 else history_titles
        
        # æ„å»ºå†å²åºåˆ—å­—ç¬¦ä¸²
        if len(history_display) == 0:
            history_str = "No previous purchases"
        else:
            history_str = " -> ".join(history_display)
        
        # ç”±äºå…¨åº“å•†å“å¤ªå¤šï¼Œæˆ‘ä»¬éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†è¿›è¡Œæ’åºï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„è®¡ç®—çº¦æŸï¼‰
        # ä½†ä¿è¯ç›®æ ‡å•†å“ä¸€å®šåœ¨å…¶ä¸­
        sample_size = min(100, len(all_available_items))  # ä¸è®­ç»ƒæ—¶çš„batch sizeç±»ä¼¼
        
        prompt = f"""You are a recommendation system for {category} products.

User's purchase history: {history_str}

Based on this history, I need you to rank {sample_size} candidate items by likelihood of purchase.

Please provide your top 20 recommendations with item numbers, ranked from most likely to least likely.
Format your response as a comma-separated list of numbers only.

Example response: 15,3,7,42,88,12,55,73,9,21,34,67,81,96,11,45,78,23,56,39
"""
        
        return prompt
    
    def parse_ranking_response(self, response: str, candidate_items: List[int]) -> List[int]:
        """
        è§£ææ’åºå“åº”ï¼Œè¿”å›æŒ‰é¢„æµ‹æ¦‚ç‡æ’åºçš„å•†å“ID
        
        Args:
            response: APIå“åº”
            candidate_items: å€™é€‰å•†å“IDåˆ—è¡¨
            
        Returns:
            æŒ‰é¢„æµ‹æ¦‚ç‡æ’åºçš„å•†å“IDåˆ—è¡¨ï¼ˆåªè¿”å›æ¨¡å‹é¢„æµ‹çš„top-Kï¼Œä¸è¡¥å……ï¼‰
        """
        try:
            # ä¼˜å…ˆæŸ¥æ‰¾ RANKING: å¼€å¤´çš„è¡Œ
            ranking_line = None
            lines = response.strip().split('\n')
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾ RANKING: å¼€å¤´çš„è¡Œ
            for line in lines:
                if line.strip().startswith('RANKING:'):
                    ranking_line = line.strip()
                    break
            
            if ranking_line:
                # æå– RANKING: åé¢çš„æ•°å­—
                ranking_part = ranking_line.split('RANKING:', 1)[1].strip()
                numbers = re.findall(r'\b\d+\b', ranking_part)
                print(f"Found RANKING line: {ranking_part}") if len(candidate_items) <= 100 else None
            else:
                # æ–¹æ³•2ï¼šæŸ¥æ‰¾æœ€åä¸€è¡ŒåŒ…å«é€—å·åˆ†éš”æ•°å­—çš„è¡Œ
                for line in reversed(lines):
                    line = line.strip()
                    if ',' in line and len(re.findall(r'\b\d+\b', line)) >= 10:
                        numbers = re.findall(r'\b\d+\b', line)
                        print(f"Found comma-separated line: {line}") if len(candidate_items) <= 100 else None
                        break
                else:
                    # æ–¹æ³•3ï¼šæå–æ‰€æœ‰æ•°å­—ä½œä¸ºå¤‡é€‰ï¼ˆæœ€åçš„fallbackï¼‰
                    numbers = re.findall(r'\b\d+\b', response)
                    print(f"Fallback: using all numbers from response") if len(candidate_items) <= 100 else None
            
            # å°†æ•°å­—æ˜ å°„å›å•†å“IDï¼ˆåªå¤„ç†æ¨¡å‹è¾“å‡ºçš„æ•°é‡ï¼Œä¸è¡¥å……ï¼‰
            predicted_item_ids = []
            invalid_indices = []
            
            for i, num_str in enumerate(numbers):
                try:
                    idx = int(num_str) - 1  # è½¬æ¢ä¸º0-indexed
                    if 0 <= idx < len(candidate_items):
                        item_id = candidate_items[idx]
                        if item_id not in predicted_item_ids:
                            predicted_item_ids.append(item_id)
                    else:
                        invalid_indices.append(num_str)
                except:
                    invalid_indices.append(num_str)
            
            if invalid_indices and len(candidate_items) <= 100:
                print(f"Invalid indices found: {invalid_indices[:5]}...")
            
            # å…³é”®ä¿®æ”¹ï¼šä¸å†è¡¥å……å‰©ä½™å•†å“ï¼
            # åªè¿”å›æ¨¡å‹å®é™…é¢„æµ‹çš„æ’åºï¼Œé€šå¸¸æ˜¯top-20
            if len(candidate_items) <= 100:
                print(f"Successfully parsed {len(predicted_item_ids)} items (model prediction), first 10: {[candidate_items.index(id)+1 for id in predicted_item_ids[:10]]}")
            
            return predicted_item_ids
            
        except Exception as e:
            print(f"Error parsing response: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯éšæœºæ’åº
            return []
    
    def calculate_metrics_like_training(self, predicted_ranking: List[int], target_id: int) -> Dict[str, float]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´
        é‡‡ç”¨ä¸trainers/utils.pyä¸­calculate_metricså‡½æ•°ç›¸åŒçš„é€»è¾‘
        
        Args:
            predicted_ranking: é¢„æµ‹çš„å•†å“IDæ’åºï¼ˆå¯èƒ½åªæœ‰top-Kä¸ªï¼Œå¦‚20ä¸ªï¼‰
            target_id: ç›®æ ‡å•†å“ID
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # æ‰¾åˆ°ç›®æ ‡å•†å“åœ¨é¢„æµ‹æ’åºä¸­çš„ä½ç½®ï¼ˆ0-indexedï¼Œä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        try:
            target_position = predicted_ranking.index(target_id)  # 0-indexed position
        except ValueError:
            # ç›®æ ‡å•†å“ä¸åœ¨é¢„æµ‹ä¸­ï¼Œè®¤ä¸ºæ’åœ¨é¢„æµ‹åˆ—è¡¨ä¹‹å
            # å¯¹äºNDCG@Kè®¡ç®—ï¼Œå¦‚æœç›®æ ‡ä¸åœ¨top-Kä¸­ï¼Œåˆ™è´¡çŒ®ä¸º0
            target_position = len(predicted_ranking)  # è®¾ä¸ºé¢„æµ‹åˆ—è¡¨é•¿åº¦ï¼Œè¡¨ç¤ºä¸åœ¨é¢„æµ‹ä¸­
        
        # è®¡ç®—Hit Rate @ Kï¼Œä¸è®­ç»ƒè„šæœ¬çš„ks=[5, 10, 20]ä¸€è‡´
        for k in [5, 10, 20]:
            # å¦‚æœç›®æ ‡åœ¨top-kä¸­ï¼Œåˆ™å‘½ä¸­
            metrics[f'hit_rate@{k}'] = 1.0 if target_position < k else 0.0
        
        # è®¡ç®—DCG @ Kï¼Œä½¿ç”¨ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ç›¸åŒçš„å…¬å¼
        # è®­ç»ƒè„šæœ¬ï¼šdiscount = torch.log2(torch.arange(2, cutoff + 2))
        # dcg = (1.0 / discount)
        # æ³¨æ„ï¼špositionä»0å¼€å§‹ï¼Œæ‰€ä»¥discountçš„ç´¢å¼•æ˜¯position+2
        for k in [5, 10, 20]:
            if target_position < k:
                # ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼šposition 0å¯¹åº”log2(2), position 1å¯¹åº”log2(3)...
                dcg_value = 1.0 / np.log2(target_position + 2)
                metrics[f'ndcg@{k}'] = dcg_value  # è®­ç»ƒè„šæœ¬æ²¡æœ‰IDCGå½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨DCG
            else:
                # ç›®æ ‡å•†å“ä¸åœ¨top-kä¸­ï¼Œè´¡çŒ®ä¸º0
                metrics[f'ndcg@{k}'] = 0.0
        
        return metrics
    
    def evaluate_like_training(self, 
                             num_samples: Optional[int] = None,  # æ”¹ä¸ºå¯é€‰ï¼Œé»˜è®¤æµ‹è¯•å…¨é›†
                             split: str = 'test',
                             save_results: bool = True,
                             progress_interval: int = 100,  # æ–°å¢ï¼šè¿›åº¦è¾“å‡ºé—´éš”
                             save_interval: int = 50,  # æ–°å¢ï¼šä¿å­˜é—´éš”
                             preserve_order: bool = False) -> Dict[str, float]:
        """
        è¯„ä¼°å‡½æ•°ï¼Œå®Œå…¨æ¨¡æ‹Ÿè®­ç»ƒè„šæœ¬ä¸­çš„è¯„ä¼°è¿‡ç¨‹
        
        Args:
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºæµ‹è¯•å…¨é›†ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
            split: æ•°æ®é›†åˆ†å‰² ('test' æˆ– 'valid')
            save_results: æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœ
            progress_interval: æ¯éš”å¤šå°‘ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼ˆé»˜è®¤100ï¼‰
            save_interval: æ¯éš”å¤šå°‘ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœï¼ˆé»˜è®¤50ï¼‰
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if self.seed is not None:
            random.seed(self.seed)
            np.random.seed(self.seed)

        data = self.test_data if split == 'test' else self.valid_data
        total_samples = len(data)
        using_predefined = bool(self.predefined_candidate_entries) and split == self.predefined_candidate_split
        if using_predefined:
            predefined_total = len(self.predefined_candidate_entries)
            if num_samples is None or num_samples > predefined_total:
                num_samples = predefined_total
            print(f"â„¹ï¸  Using predefined candidate JSON for {split} split with {predefined_total} available entries")
        
        # å¦‚æœnum_samplesä¸ºNoneï¼Œæµ‹è¯•å…¨é›†ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        if num_samples is None:
            num_samples = total_samples
            print(f"\n=== Evaluating API like Training Script on {split} set (FULL DATASET: {total_samples} samples) ===")
        else:
            num_samples = min(num_samples, total_samples)
            print(f"\n=== Evaluating API like Training Script on {split} set ({num_samples}/{total_samples} samples, preserve_order={preserve_order}) ===")
        
        print(f"ğŸ’¾ Incremental save every {save_interval} samples")
        category = self.dataset_dir.split('/')[-1].split('_')[0].replace('_', ' ')

        prepared_samples = self.prepare_rank_candidates(data, num_samples, split, preserve_order=preserve_order)
        if not prepared_samples:
            print(f"âŒ No valid user samples available for {split} evaluation")
            return {}

        total_to_evaluate = len(prepared_samples)
        candidate_count = len(prepared_samples[0]['candidate_items']) if prepared_samples else 0
        
        # åˆå§‹åŒ–ç´¯ç§¯æŒ‡æ ‡ï¼Œä¸è®­ç»ƒè„šæœ¬çš„MetricUpdaterä¸€è‡´
        accumulated_metrics = {
            'hit_rate@5': 0.0, 'hit_rate@10': 0.0, 'hit_rate@20': 0.0,
            'ndcg@5': 0.0, 'ndcg@10': 0.0, 'ndcg@20': 0.0
        }
        
        valid_samples = 0
        detailed_results = []
        prompt_response_logs = []  # æ–°å¢ï¼šä¿å­˜promptå’Œresponseç”¨äºè°ƒè¯•
        
        for prepared in tqdm(prepared_samples, desc=f"Evaluating {split}"):
            sample = prepared['sample']
            history_ids = prepared['history_ids']
            history_titles = prepared['history_titles']
            target_id = prepared['target_id']
            target_title = prepared['target_title']
            candidate_items = prepared['candidate_items']
            sample_index = prepared['sample_index']
            user_id = prepared['user_id']
            target_candidate_position = prepared.get('target_candidate_position')
            
            # æ„å»ºå€™é€‰å•†å“æ ‡é¢˜åˆ—è¡¨ç”¨äºprompt
            candidate_titles = [self.id2title[item_id] for item_id in candidate_items]
            candidates_str = "\n".join([f"{i+1}. {title}" for i, title in enumerate(candidate_titles)])
            
            # åˆ›å»ºpromptï¼Œæ˜ç¡®æŒ‡å®šè¾“å‡ºæ ¼å¼
            prompt = f"""You are a recommendation system for {category} products.

User's purchase history: {" -> ".join(history_titles[-20:])}

Please rank the following {len(candidate_items)} items by likelihood of purchase.

Candidate items:
{candidates_str}

IMPORTANT: Your response must end with exactly one line in this format:
RANKING: number1,number2,number3,number4,number5,number6,number7,number8,number9,number10,number11,number12,number13,number14,number15,number16,number17,number18,number19,number20

Where each number is between 1-{len(candidate_items)}. Example:
RANKING: 26,45,78,50,38,99,77,43,53,89,8,93,97,52,47,31,48,83,98,79
"""
            
            # è°ƒç”¨API
            response, reasoning, raw_response = self.call_api(prompt)
            
            # è§£æå“åº”è·å¾—æ’åº
            predicted_ranking = self.parse_ranking_response(response, candidate_items)
            
            # æ›´ç²¾ç¡®çš„è§£ææˆåŠŸç‡æ£€æŸ¥
            # parsing_success ç°åœ¨è¡¨ç¤ºæ˜¯å¦æˆåŠŸè§£æå‡ºäº†æ¨èåˆ—è¡¨ï¼ˆä¸è¦æ±‚å®Œæ•´ï¼‰
            parsing_success = (len(predicted_ranking) > 0)
            
            # è®¡ç®—ç›®æ ‡å•†å“åœ¨é¢„æµ‹ä¸­çš„æ’åï¼ˆ1-indexed for displayï¼‰
            if target_id in predicted_ranking:
                target_rank_in_prediction = predicted_ranking.index(target_id) + 1
            else:
                # ç›®æ ‡ä¸åœ¨é¢„æµ‹çš„top-Kä¸­ï¼Œè®¾ä¸ºé¢„æµ‹é•¿åº¦+1ï¼ˆè¡¨ç¤ºæ’åœ¨é¢„æµ‹åˆ—è¡¨å¤–ï¼‰
                target_rank_in_prediction = len(predicted_ranking) + 1
            
            # è®¡ç®—æŒ‡æ ‡
            sample_metrics = self.calculate_metrics_like_training(predicted_ranking, target_id)
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in accumulated_metrics:
                accumulated_metrics[key] += sample_metrics[key]
            
            valid_samples += 1
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            if save_results:
                detailed_results.append({
                    'sample_id': sample_index,
                    'user_id': user_id,
                    'history_titles': history_titles[-10:],
                    'target_title': target_title,
                    'target_rank': target_rank_in_prediction,
                    'predicted_items_count': len(predicted_ranking),  # æ–°å¢ï¼šå®é™…é¢„æµ‹çš„å•†å“æ•°é‡
                    'total_candidates': len(candidate_items),
                    'parsing_success': parsing_success,
                    'target_candidate_position': target_candidate_position,
                    'metrics': sample_metrics,
                    'api_response': response[:1000],  # æˆªæ–­ä¿å­˜
                    'reasoning_preview': reasoning[:1000] if reasoning else None  # æ–°å¢ï¼šæ€è€ƒè¿‡ç¨‹é¢„è§ˆ
                })
                
                # ä¿å­˜è¯¦ç»†çš„promptå’Œresponseç”¨äºè°ƒè¯•
                prompt_response_logs.append({
                    'sample_id': sample_index,
                    'user_id': user_id,
                    'prompt': prompt,
                    'response': response,
                    'reasoning': reasoning,  # æ–°å¢ï¼šå®Œæ•´çš„æ€è€ƒè¿‡ç¨‹
                    'raw_response': raw_response,  # æ–°å¢ï¼šåŸå§‹å“åº”ç”¨äºè°ƒè¯•
                    'candidate_items': candidate_items,
                    'predicted_ranking': predicted_ranking,
                    'target_id': target_id,
                    'target_rank': target_rank_in_prediction,
                    'target_candidate_position': target_candidate_position,
                    'parsing_success': parsing_success,
                    'sample_metrics': sample_metrics
                })
            
            # å®æ—¶è¿›åº¦æ˜¾ç¤ºï¼šæ¯progress_intervalä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡ä¸­é—´ç»“æœ
            if valid_samples % progress_interval == 0:
                current_metrics = {k: v / valid_samples for k, v in accumulated_metrics.items()}
                print(f"\n--- Progress Update: {valid_samples}/{total_to_evaluate} samples ---")
                print(f"Current NDCG@10: {current_metrics['ndcg@10']:.6f}")
                print(f"Current Hit Rate@10: {current_metrics['hit_rate@10']:.6f}")
                print(f"Target found in top-10: {sum(1 for r in detailed_results[-progress_interval:] if r['target_rank'] <= 10)}/{min(progress_interval, len(detailed_results))} samples")
                print(f"Parsing success rate: {sum(1 for r in detailed_results[-progress_interval:] if r['parsing_success'])}/{min(progress_interval, len(detailed_results))} samples")
                print("-" * 50)
            
            # å¢é‡ä¿å­˜ï¼šæ¯save_intervalä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if save_results and valid_samples % save_interval == 0:
                self._save_incremental_results(
                    accumulated_metrics, detailed_results, prompt_response_logs,
                    valid_samples, split, total_to_evaluate, progress_interval,
                    preserve_order
                )
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼Œä¸è®­ç»ƒè„šæœ¬çš„MetricUpdater.compute()ä¸€è‡´
        if valid_samples > 0:
            for key in accumulated_metrics:
                accumulated_metrics[key] = accumulated_metrics[key] / valid_samples
        
        accumulated_metrics['total_samples'] = valid_samples
        
        # æ‰“å°æœ€ç»ˆç»“æœï¼Œæ ¼å¼ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´
        print(f"\n{'='*60}")
        print(f"FINAL EVALUATION RESULTS ({valid_samples} samples)")
        print(f"{'='*60}")
        print(f"hit_rate@5:  {accumulated_metrics['hit_rate@5']:.6f}")
        print(f"hit_rate@10: {accumulated_metrics['hit_rate@10']:.6f}")
        print(f"hit_rate@20: {accumulated_metrics['hit_rate@20']:.6f}")
        print(f"ndcg@5:      {accumulated_metrics['ndcg@5']:.6f}")
        print(f"ndcg@10:     {accumulated_metrics['ndcg@10']:.6f}")  # ä¸è®­ç»ƒæ—¥å¿—æ ¼å¼ä¸€è‡´
        print(f"ndcg@20:     {accumulated_metrics['ndcg@20']:.6f}")
        
        # è®¡ç®—å’Œæ˜¾ç¤ºé¢å¤–çš„åˆ†ææŒ‡æ ‡
        if detailed_results:
            target_in_top10_count = sum(1 for r in detailed_results if r['target_rank'] <= 10)
            parsing_success_count = sum(1 for r in detailed_results if r['parsing_success'])
            avg_predicted_items = sum(r['predicted_items_count'] for r in detailed_results) / len(detailed_results)
            print(f"\nAdditional Analysis:")
            print(f"Target in Top-10: {target_in_top10_count}/{len(detailed_results)} ({target_in_top10_count/len(detailed_results)*100:.1f}%)")
            print(f"Parsing Success: {parsing_success_count}/{len(detailed_results)} ({parsing_success_count/len(detailed_results)*100:.1f}%)")
            print(f"Average Predicted Items: {avg_predicted_items:.1f} (vs {candidate_count} candidates)")
            target_positions = [r['target_candidate_position'] for r in detailed_results if r.get('target_candidate_position')]
            if target_positions:
                avg_target_position = sum(target_positions) / len(target_positions)
                print(f"Average Target Position in Candidates: {avg_target_position:.1f} ({self.target_position_mode})")
            
            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
            print(f"\nExample predictions (first 3 samples):")
            for i, result in enumerate(detailed_results[:3]):
                print(f"  Sample {result['sample_id']}: Target rank {result['target_rank']}/{result['predicted_items_count']} predicted, NDCG@10={result['metrics']['ndcg@10']:.4f}")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            # ä¿å­˜ä¸»è¦ç»“æœ
            results_file = os.path.join(
                self.output_dir,
                f"deepseek_api_results_{split}_{self.model_name.replace('/', '_')}.json"
            )
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'metrics': accumulated_metrics,
                    'detailed_results': detailed_results,
                    'config': {
                        'dataset_dir': self.dataset_dir,
                        'model_name': self.model_name,
                        'num_samples': total_to_evaluate,
                        'split': split,
                        'evaluation_method': 'training_consistent',
                        'api_base': self.api_base,
                        'progress_interval': progress_interval,
                        'seed': self.seed,
                        'target_position_mode': self.target_position_mode,
                        'output_dir': self.output_dir,
                        'test_sample_limit': self.test_limit,
                        'preserve_order': preserve_order
                    }
                }, f, indent=2, ensure_ascii=False)
            print(f"\nDetailed results saved to {results_file}")
            
            # ä¿å­˜promptå’Œresponseæ—¥å¿—ç”¨äºè°ƒè¯•
            if prompt_response_logs:
                debug_log_file = os.path.join(
                    self.output_dir,
                    f"api_debug_logs_{split}_{self.model_name.replace('/', '_')}.json"
                )
                with open(debug_log_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'logs': prompt_response_logs,
                        'summary': {
                            'total_samples': len(prompt_response_logs),
                            'parsing_success_rate': sum(1 for log in prompt_response_logs if log['parsing_success']) / len(prompt_response_logs),
                            'avg_target_rank': sum(log['target_rank'] for log in prompt_response_logs) / len(prompt_response_logs),
                            'config': {
                                'dataset_dir': self.dataset_dir,
                                'model_name': self.model_name,
                                'split': split,
                                'target_position_mode': self.target_position_mode,
                                'output_dir': self.output_dir,
                                'test_sample_limit': self.test_limit,
                                'preserve_order': preserve_order
                            }
                        }
                    }, f, indent=2, ensure_ascii=False)
                print(f"Debug logs (prompts & responses) saved to {debug_log_file}")
        
        return accumulated_metrics
    
    def _save_incremental_results(self, accumulated_metrics, detailed_results, prompt_response_logs,
                                 valid_samples, split, num_samples, progress_interval,
                                 preserve_order: bool = False):
        """å¢é‡ä¿å­˜ä¸­é—´ç»“æœï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­å¯¼è‡´æ•°æ®ä¸¢å¤±"""
        try:
            # è®¡ç®—å½“å‰å¹³å‡æŒ‡æ ‡
            current_metrics = {k: v / valid_samples for k, v in accumulated_metrics.items()}
            current_metrics['total_samples'] = valid_samples
            
            # ä¿å­˜ä¸­é—´ç»“æœæ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            import time
            timestamp = int(time.time())
            
            # ä¸»è¦ç»“æœçš„å¢é‡ä¿å­˜
            incremental_file = os.path.join(
                self.output_dir,
                f"api_incremental_{split}_{self.model_name.replace('/', '_')}_{valid_samples}samples_{timestamp}.json"
            )
            with open(incremental_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'current_metrics': current_metrics,
                    'detailed_results': detailed_results,
                    'status': {
                        'completed_samples': valid_samples,
                        'total_samples': num_samples,
                        'progress_percentage': (valid_samples / num_samples * 100) if num_samples else 0,
                        'timestamp': timestamp,
                        'is_incremental': True
                    },
                    'config': {
                        'dataset_dir': self.dataset_dir,
                        'model_name': self.model_name,
                        'split': split,
                        'evaluation_method': 'training_consistent',
                        'api_base': self.api_base,
                        'progress_interval': progress_interval,
                        'seed': self.seed,
                        'target_position_mode': self.target_position_mode,
                        'output_dir': self.output_dir,
                        'test_sample_limit': self.test_limit,
                        'preserve_order': preserve_order
                    }
                }, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ Incremental save: {incremental_file} ({valid_samples} samples)")
            
            # ä¿å­˜æœ€æ–°çš„è°ƒè¯•æ—¥å¿—ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
            if prompt_response_logs:
                debug_incremental_file = os.path.join(
                    self.output_dir,
                    f"api_debug_incremental_{split}_{self.model_name.replace('/', '_')}.json"
                )
                with open(debug_incremental_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'logs': prompt_response_logs,
                        'summary': {
                            'total_samples': len(prompt_response_logs),
                            'parsing_success_rate': sum(1 for log in prompt_response_logs if log['parsing_success']) / len(prompt_response_logs),
                            'avg_target_rank': sum(log['target_rank'] for log in prompt_response_logs) / len(prompt_response_logs),
                            'timestamp': timestamp,
                            'is_incremental': True,
                            'target_position_mode': self.target_position_mode,
                            'output_dir': self.output_dir,
                            'test_sample_limit': self.test_limit,
                            'preserve_order': preserve_order
                        }
                    }, f, indent=2, ensure_ascii=False)
        
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to save incremental results: {e}")
    
    def run_full_evaluation(self, 
                          test_samples: Optional[int] = None,  # æ”¹ä¸ºå¯é€‰ï¼Œé»˜è®¤å…¨æµ‹è¯•é›†
                          valid_samples: Optional[int] = None,  # æ”¹ä¸ºå¯é€‰ï¼Œé»˜è®¤å…¨éªŒè¯é›†
                          progress_interval: int = 100,  # æ·»åŠ è¿›åº¦é—´éš”å‚æ•°
                          save_interval: int = 50,  # æ–°å¢ï¼šä¿å­˜é—´éš”å‚æ•°
                          test_only: bool = False):  # æ–°å¢ï¼šä»…æµ‹è¯•æµ‹è¯•é›†
        """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šéƒ½è¿›è¡Œæµ‹è¯•ï¼Œä¸è®­ç»ƒè„šæœ¬ä¸€è‡´"""
        print("=" * 60)
        print(f"API Recommendation System Evaluation (Training Consistent)")
        print(f"Dataset: {self.dataset_dir}")
        print(f"Model: {self.model_name}")
        print(f"API Base: {self.api_base}")
        print(f"Evaluation Method: Full item library ranking (like training)")
        print(f"Progress interval: Every {progress_interval} samples")
        print(f"Save interval: Every {save_interval} samples")
        print(f"Target position mode: {self.target_position_mode}")
        print(f"Output directory: {self.output_dir}")
        if self.test_limit is not None:
            print(f"Test sample limit (first-N in order when applicable): {self.test_limit}")
        if test_only:
            print(f"ğŸ’° Cost-saving mode: Testing ONLY the test set")
        print("=" * 60)
        
        self.test_only_mode = test_only
        preserve_order_for_test = test_only and self.test_limit is not None

        valid_metrics = None
        
        # éªŒè¯é›†è¯„ä¼°ï¼ˆä»…åœ¨étest_onlyæ¨¡å¼ä¸‹ï¼‰
        if not test_only:
            print("\n1. Validation Set Evaluation")
            valid_metrics = self.evaluate_like_training(
                num_samples=valid_samples,  # Noneè¡¨ç¤ºå…¨é›†
                split='valid',
                progress_interval=progress_interval,  # ä½¿ç”¨ä¼ å…¥çš„è¿›åº¦é—´éš”
                save_interval=save_interval,  # ä½¿ç”¨ä¼ å…¥çš„ä¿å­˜é—´éš”
                preserve_order=False
            )
        else:
            print("\nâ­ï¸  Skipping validation set evaluation (test_only mode)")
        
        # æµ‹è¯•é›†è¯„ä¼°
        eval_number = "1" if test_only else "2"
        print(f"\n{eval_number}. Test Set Evaluation")
        requested_test_samples = self.test_limit if preserve_order_for_test else test_samples
        test_metrics = self.evaluate_like_training(
            num_samples=requested_test_samples,  # Noneè¡¨ç¤ºå…¨é›†
            split='test',
            progress_interval=progress_interval,  # ä½¿ç”¨ä¼ å…¥çš„è¿›åº¦é—´éš”
            save_interval=save_interval,  # ä½¿ç”¨ä¼ å…¥çš„ä¿å­˜é—´éš”
            preserve_order=preserve_order_for_test
        )
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "=" * 60)
        print("FINAL RESULTS SUMMARY (Training Consistent)")
        print("=" * 60)
        
        if valid_metrics:
            print("\nValidation Set:")
            for metric, value in valid_metrics.items():
                if metric != 'total_samples':
                    print(f"  {metric}: {value:.4f}")
        
        print("\nTest Set:")
        for metric, value in test_metrics.items():
            if metric != 'total_samples':
                print(f"  {metric}: {value:.4f}")
        
        # ä¸è®­ç»ƒæ—¥å¿—çš„ç›®æ ‡æŒ‡æ ‡å¯¹æ¯”
        train_target_ndcg = 0.012233516966979588
        test_ndcg = test_metrics.get('ndcg@10', 0)
        print(f"\nğŸ“Š Comparison with training target:")
        print(f"  Training model ndcg@10: {train_target_ndcg:.6f}")
        print(f"  {self.model_name} ndcg@10:  {test_ndcg:.6f}")
        print(f"  Difference: {test_ndcg - train_target_ndcg:+.6f}")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_results = {
            'test': test_metrics,
            'config': {
                'dataset_dir': self.dataset_dir,
                'model_name': self.model_name,
                'api_base': self.api_base,
                'test_samples_requested': requested_test_samples,
                'test_samples_evaluated': test_metrics.get('total_samples'),
                'evaluation_method': 'training_consistent',
                'test_only_mode': test_only,
                'seed': self.seed,
                'target_position_mode': self.target_position_mode,
                'output_dir': self.output_dir,
                'test_sample_limit': self.test_limit,
                'preserve_order_for_test': preserve_order_for_test
            }
        }
        
        if valid_metrics:
            summary_results['validation'] = valid_metrics
            summary_results['config']['valid_samples_requested'] = valid_samples
            summary_results['config']['valid_samples_evaluated'] = valid_metrics.get('total_samples')
        
        summary_file = os.path.join(
            self.output_dir,
            f"api_consistent_summary_{self.model_name.replace('/', '_')}.json"
        )
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_results, f, indent=2, ensure_ascii=False)
        print(f"\nSummary results saved to {summary_file}")
        
        if test_only:
            return test_metrics
        else:
            return valid_metrics, test_metrics

def main():
    parser = argparse.ArgumentParser(description="Test API with training-consistent evaluation")
    parser.add_argument("--api_key", type=str, default="sk-ZUK05LeE0U3FVQAp6e5eCd3628774c4181D513786bD3B901", help="API key")
    parser.add_argument("--dataset_dir", type=str, default="/data/hongdeyao/Movies_and_TV_0_2022-10-2023-10", help="Dataset directory")
    parser.add_argument("--test_samples", type=int, default=200, help="Number of test samples (None for full dataset)")
    parser.add_argument("--test_limit", type=int, default=None,
                        help="In test-only mode, evaluate only the first N samples from the test split")
    parser.add_argument("--valid_samples", type=int, default=None, help="Number of validation samples (None for full dataset)")
    parser.add_argument("--api_base", type=str, default="http://115.182.62.174:18888/v1", help="API base URL")
    parser.add_argument("--model_name", type=str, default="deepseek/deepseek-r1", 
                       choices=["deepseek/deepseek-r1"],
                       help="Model name")
    parser.add_argument("--sample_for_api_efficiency", action="store_true", 
                       help="Use sampling for API efficiency (default: False, test full dataset like training)")
    parser.add_argument("--progress_interval", type=int, default=2, 
                       help="Show progress every N samples (default: 50)")
    parser.add_argument("--save_interval", type=int, default=5, 
                       help="Save incremental results every N samples (default: 25)")
    parser.add_argument("--test_only", action="store_true", 
                       help="Only evaluate test set (saves API costs)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for candidate sampling")
    parser.add_argument("--target_position", type=str, default="random",
                        choices=["first", "last", "random"],
                        help="Control where the correct item appears in candidates (default: first)")
    parser.add_argument("--output_dir", type=str, default="/data/hongdeyao/code/RRec/sft/outputs/movies_200_sample",
                        help="Directory to save evaluation outputs (default: current directory)")
    parser.add_argument("--candidate_mode", type=str, default="json",
                        choices=["random", "json"],
                        help="Candidate selection mode: random sampling or predefined JSON")
    parser.add_argument("--candidate_json_path", type=str, default="/data/hongdeyao/code/RRec/sft/outputs/movies_200_sample/checkpoint-009000.json",
                        help="Path to predefined candidate JSON file when candidate_mode=json")
    parser.add_argument("--candidate_json_split", type=str, default="test",
                        choices=["train", "valid", "test"],
                        help="Dataset split to pair with the predefined candidate JSON entries")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
    random.seed(args.seed)
    np.random.seed(args.seed)
    
    # å¦‚æœå¯ç”¨é‡‡æ ·æ¨¡å¼ï¼Œè®¾ç½®é»˜è®¤æ ·æœ¬æ•°
    if args.sample_for_api_efficiency:
        test_samples = args.test_samples or 1000
        valid_samples = args.valid_samples or 500
        print("âš ï¸  API efficiency mode enabled: using sampling instead of full dataset")
    else:
        test_samples = args.test_samples  # Noneè¡¨ç¤ºå…¨é›†
        valid_samples = args.valid_samples  # Noneè¡¨ç¤ºå…¨é›†
        print("âœ… Full dataset evaluation mode (same as training script)")

    if args.test_limit is not None and not args.test_only:
        print("â„¹ï¸  test_limit is set but test_only mode is disabled; limit will apply only when test_only is True.")

    if args.candidate_mode == "json":
        candidate_json_path = os.path.abspath(args.candidate_json_path)
        print(f"ğŸ“„ Candidate selection mode: JSON (path={candidate_json_path})")
    else:
        candidate_json_path = None
        print("ğŸ² Candidate selection mode: random sampling from dataset")
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = APIRecommendationTester(
        api_key=args.api_key,
        dataset_dir=args.dataset_dir,
        api_base=args.api_base,
        model_name=args.model_name,
        seed=args.seed,
        target_position=args.target_position,
        output_dir=args.output_dir,
        test_limit=args.test_limit,
        predefined_candidates_path=candidate_json_path,
        predefined_candidate_split=args.candidate_json_split
    )
    
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    tester.run_full_evaluation(
        test_samples=test_samples,
        valid_samples=valid_samples,
        progress_interval=args.progress_interval,
        save_interval=args.save_interval,
        test_only=args.test_only
    )

if __name__ == "__main__":
    main()
