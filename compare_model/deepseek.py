import os
import json
import numpy as np
import pandas as pd
from datasets import load_from_disk
from tqdm import tqdm
from typing import List, Dict, Any, Optional
import time
import argparse
from openai import OpenAI
import random
import re

# --test_only

class APIRecommendationTester:
    VALID_TARGET_POSITIONS = {"first", "last", "random"}

    def __init__(self, 
                 api_key: str, 
                 dataset_dir: str,
                 api_base: str = "http://115.182.62.174:18888/v1",
                 model_name: str = "deepseek-reasoner",
                 seed: int = 42,
                 target_position: str = "first",
                 output_dir: str = ".",
                 test_limit: Optional[int] = None,
                 predefined_candidates_path: Optional[str] = None,
                 predefined_candidate_split: str = "test"):
        """
        ÂàùÂßãÂåñAPIÊµãËØïÂô®Ôºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÂÆåÂÖ®‰∏ÄËá¥ÁöÑËØÑ‰º∞ÊñπÂºè
        
        Args:
            api_key: APIÂØÜÈí•
            dataset_dir: Êï∞ÊçÆÈõÜÁõÆÂΩïË∑ØÂæÑ
            api_base: APIÂü∫Á°ÄURL
            model_name: ‰ΩøÁî®ÁöÑÊ®°ÂûãÂêçÁß∞
            seed: ÈöèÊú∫ÁßçÂ≠ê
            target_position: Ê≠£Á°ÆÂÄôÈÄâÂú®ÂÄôÈÄâÂàóË°®‰∏≠ÁöÑ‰ΩçÁΩÆÁ≠ñÁï•Ôºàfirst/last/randomÔºâ
            output_dir: ÁªìÊûúÊñá‰ª∂‰øùÂ≠òÁõÆÂΩï
            test_limit: test-onlyÊ®°Âºè‰∏ãËØÑ‰º∞ÁöÑÊúÄÂ§ßÊ†∑Êú¨Êï∞ÈáèÔºàÊåâÊï∞ÊçÆÈ°∫Â∫èÔºâ
        """
        self.api_key = api_key
        self.dataset_dir = dataset_dir
        self.api_base = api_base
        self.model_name = model_name
        self.seed = seed
        self.target_position_mode = target_position.lower()
        if self.target_position_mode not in self.VALID_TARGET_POSITIONS:
            raise ValueError(f"target_position must be one of {sorted(self.VALID_TARGET_POSITIONS)}, got {target_position}")
        self.output_dir = os.path.abspath(output_dir)
        os.makedirs(self.output_dir, exist_ok=True)
        self.test_limit = test_limit
        self.test_only_mode = False
        self.predefined_candidate_split = predefined_candidate_split.lower() if predefined_candidate_split else "test"
        self.predefined_candidates_path = predefined_candidates_path
        self.predefined_candidate_entries: Optional[List[Dict[str, Any]]] = None
        self.predefined_candidate_config: Dict[str, Any] = {}
        self._split_user_index_cache: Dict[str, Dict[str, List[int]]] = {}
        
        # ÂàùÂßãÂåñOpenAIÂÆ¢Êà∑Á´Ø
        self.client = OpenAI(base_url=api_base, api_key=api_key)
        
        # Âä†ËΩΩÊï∞ÊçÆÈõÜ
        self.load_dataset()

        if self.predefined_candidates_path:
            self.load_predefined_candidates(self.predefined_candidates_path)

    def prepare_rank_candidates(self, 
                                 data_split: List[Dict[str, Any]],
                                 num_users: Optional[int],
                                 split_name: str,
                                 preserve_order: bool = False) -> List[Dict[str, Any]]:
        """‰ΩøÁî®‰∏érun_rrec.pyÂíågenerate_prompts_rank.py‰∏ÄËá¥ÁöÑÊñπÂºèÊûÑÂª∫ÂÄôÈÄâÈõÜÂêà"""
        if self.predefined_candidate_entries and split_name == self.predefined_candidate_split:
            return self.prepare_rank_candidates_from_predefined(data_split, num_users, split_name)
        order_note = "preserving dataset order" if preserve_order else "with shuffled users"
        print(f"\nüîÑ Preparing rank candidates for {split_name} split using training-consistent sampler ({order_note})")

        user_samples: Dict[Any, List[Dict[str, Any]]] = {}
        for idx in range(len(data_split)):
            sample = data_split[idx]
            user_id = sample.get('user_id')
            if user_id is None:
                continue
            if user_id not in user_samples:
                user_samples[user_id] = []
            user_samples[user_id].append({'index': idx, 'sample': sample})

        available_users = list(user_samples.keys())
        if not preserve_order:
            random.shuffle(available_users)

        if not available_users:
            print(f"‚ùå Error: No valid users found in {split_name} split")
            return []

        requested_users = num_users if num_users is not None else len(available_users)
        if requested_users > len(available_users):
            print(f"‚ö†Ô∏è Warning: Requested {requested_users} users but only {len(available_users)} available in {split_name}, adjusting")
            requested_users = len(available_users)

        prepared_samples: List[Dict[str, Any]] = []

        for user_id in available_users:
            if len(prepared_samples) >= requested_users:
                break

            entries = user_samples[user_id]
            selected_entry: Optional[Dict[str, Any]] = None
            for entry in entries:
                target_id = entry['sample'].get('item_id', 0)
                if target_id and target_id in self.id2title:
                    selected_entry = entry
                    break

            if selected_entry is None:
                print(f"Warning: No valid sample found for user {user_id}, skipping")
                continue

            sample_idx = selected_entry['index']
            sample = selected_entry['sample']

            history_ids = [hid for hid in sample['history_item_id'] if hid != 0]
            # if len(history_ids) < 5:
            #     print(f"Warning: User {user_id} has only {len(history_ids)} history items, skipping")
            #     continue

            target_id = sample['item_id']
            if target_id == 0:
                print(f"Warning: Sample {sample_idx} for user {user_id} has padding target, skipping")
                continue

            available_items = [item_id for item_id in self.all_item_ids if item_id not in history_ids]
            if target_id not in available_items:
                available_items.append(target_id)

            wrong_pool = [item_id for item_id in available_items if item_id != target_id]
            if len(wrong_pool) < 19:
                print(f"Warning: User {user_id} only has {len(wrong_pool)} wrong candidates available, skipping")
                continue

            wrong_candidates = random.sample(wrong_pool, 19)
            candidate_items = wrong_candidates.copy()

            if self.target_position_mode == "first":
                candidate_items.insert(0, target_id)
            elif self.target_position_mode == "last":
                candidate_items.append(target_id)
            else:
                insert_idx = random.randint(0, len(candidate_items))
                candidate_items.insert(insert_idx, target_id)

            history_titles = [self.id2title.get(hid, f"Unknown_{hid}") for hid in history_ids]
            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")

            prepared_samples.append({
                'sample_index': sample_idx,
                'sample': sample,
                'user_id': user_id,
                'history_ids': history_ids,
                'history_titles': history_titles,
                'target_id': target_id,
                'target_title': target_title,
                'candidate_items': candidate_items,
                'target_candidate_position': candidate_items.index(target_id) + 1
            })

            print(f"‚úÖ Prepared candidate set for user {user_id} (sample {sample_idx}) in {split_name}")

        if len(prepared_samples) < requested_users:
            print(f"‚ö†Ô∏è Warning: Only prepared {len(prepared_samples)} user samples (requested {requested_users}) for {split_name}")

        return prepared_samples

    def prepare_rank_candidates_from_predefined(self,
                                                data_split: List[Dict[str, Any]],
                                                num_users: Optional[int],
                                                split_name: str) -> List[Dict[str, Any]]:
        """‰ΩøÁî®È¢ÑÂÆö‰πâÁöÑJSONÂÄôÈÄâÈõÜÂêàÊù•ÊûÑÂª∫ËØÑ‰º∞Ê†∑Êú¨"""
        print(f"\nüóÇÔ∏è  Preparing rank candidates for {split_name} split using predefined JSON entries (preserving file order)")

        if not self.predefined_candidate_entries:
            print("‚ö†Ô∏è  Warning: No predefined entries available; returning empty list")
            return []

        requested_users = num_users if num_users is not None else len(self.predefined_candidate_entries)
        if requested_users > len(self.predefined_candidate_entries):
            print(f"‚ö†Ô∏è  Warning: Requested {requested_users} entries but only {len(self.predefined_candidate_entries)} available; adjusting")
            requested_users = len(self.predefined_candidate_entries)

        user_index_map = self._build_split_user_index(data_split, split_name)

        prepared_samples: List[Dict[str, Any]] = []

        for entry_idx, entry in enumerate(self.predefined_candidate_entries[:requested_users]):
            sample_index_raw = entry.get('sample_index')
            user_id = entry.get('user_id')
            target_id_raw = entry.get('target_item_id')
            candidate_ids = entry.get('candidate_ids') or []

            if not candidate_ids:
                print(f"‚ö†Ô∏è  Warning: Entry #{entry_idx} missing candidate IDs, skipping")
                continue

            sample_index: Optional[int] = None
            if sample_index_raw is not None:
                try:
                    sample_index = int(sample_index_raw)
                except (TypeError, ValueError):
                    sample_index = None

            target_id: Optional[int] = None
            if target_id_raw is not None:
                try:
                    target_id = int(target_id_raw)
                except (TypeError, ValueError):
                    target_id = None

            sample = None
            resolved_sample_index: Optional[int] = None

            if sample_index is not None and 0 <= sample_index < len(data_split):
                candidate_sample = data_split[sample_index]
                if user_id is None or candidate_sample.get('user_id') == user_id:
                    sample = candidate_sample
                    resolved_sample_index = sample_index

            if sample is None and user_id is not None:
                candidate_indices = user_index_map.get(user_id, [])
                for idx in candidate_indices:
                    candidate_sample = data_split[idx]
                    if target_id is None or candidate_sample.get('item_id') == target_id:
                        sample = candidate_sample
                        resolved_sample_index = idx
                        break
                if sample is None and candidate_indices:
                    sample = data_split[candidate_indices[0]]
                    resolved_sample_index = candidate_indices[0]

            if sample is None:
                print(f"‚ö†Ô∏è  Warning: Unable to locate dataset sample for entry #{entry_idx} (user_id={user_id}, sample_index={sample_index}), skipping")
                continue

            history_ids = [hid for hid in sample['history_item_id'] if hid != 0]
            history_titles = [self.id2title.get(hid, f"Unknown_{hid}") for hid in history_ids]
            dataset_target_id = sample.get('item_id', 0)

            if target_id in (None, 0):
                target_id = dataset_target_id

            if target_id == 0:
                print(f"‚ö†Ô∏è  Warning: Entry #{entry_idx} has invalid target item (0), skipping")
                continue

            candidate_items = list(dict.fromkeys(candidate_ids))  # ÂéªÈáç‰∏î‰øùÊåÅÈ°∫Â∫è
            if target_id not in candidate_items:
                candidate_items.append(target_id)
                print(f"‚ÑπÔ∏è  Info: Target item {target_id} not in candidate list for entry #{entry_idx}; appended to the end")

            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")

            try:
                target_candidate_position = candidate_items.index(target_id) + 1
            except ValueError:
                target_candidate_position = None

            prepared_samples.append({
                'sample_index': resolved_sample_index if resolved_sample_index is not None else -1,
                'sample': sample,
                'user_id': user_id or sample.get('user_id'),
                'history_ids': history_ids,
                'history_titles': history_titles,
                'target_id': target_id,
                'target_title': target_title,
                'candidate_items': candidate_items,
                'target_candidate_position': target_candidate_position
            })

            print(f"‚úÖ Prepared JSON candidate set for user {user_id or sample.get('user_id')} (entry #{entry_idx}, dataset index {resolved_sample_index})")

        if len(prepared_samples) < requested_users:
            print(f"‚ö†Ô∏è  Warning: Only prepared {len(prepared_samples)} JSON-defined samples (requested {requested_users}) for {split_name}")

        return prepared_samples
        
    def load_dataset(self):
        """Âä†ËΩΩÊï∞ÊçÆÈõÜÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÂÆåÂÖ®‰∏ÄËá¥"""
        print(f"Loading dataset from {self.dataset_dir}")
        try:
            self.dataset = load_from_disk(self.dataset_dir)
            self.test_data = self.dataset['test']
            self.valid_data = self.dataset['valid']
            self.train_data = self.dataset['train']
            self.item_info = self.dataset['item_info']
            
            # ÂàõÂª∫itemÊò†Â∞ÑÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥
            self.create_item_mappings()
            
            print(f"Dataset loaded successfully:")
            print(f"  Train samples: {len(self.train_data)}")
            print(f"  Valid samples: {len(self.valid_data)}")
            print(f"  Test samples: {len(self.test_data)}")
            print(f"  Items: {len(self.item_info)}")
            
        except Exception as e:
            print(f"Error loading dataset: {e}")
            raise
    
    def create_item_mappings(self):
        """ÂàõÂª∫ÂïÜÂìÅÊò†Â∞ÑÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÂÆåÂÖ®‰∏ÄËá¥"""
        self.id2title = {}
        self.id2asin = {}
        self.asin2title = {}
        self.all_item_ids = []
        
        for item in self.item_info:
            item_id = item['item_id']
            title = item.get('title', f'Unknown Item {item_id}')
            asin = item.get('parent_asin', f'unknown_asin_{item_id}')
            
            self.id2title[item_id] = title
            self.id2asin[item_id] = asin
            self.asin2title[asin] = title
            
            if item_id != 0:  # ÊéíÈô§padding tokenÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥
                self.all_item_ids.append(item_id)
        
        print(f"Created mappings for {len(self.id2title)} items (including {len(self.all_item_ids)} non-pad items)")

    def load_predefined_candidates(self, path: str) -> None:
        """Âä†ËΩΩÈ¢ÑÂÆö‰πâÁöÑÂÄôÈÄâÈõÜÂêàÔºåÁî®‰∫éJSONÊ®°ÂºèÁöÑÊï∞ÊçÆÊäΩÂèñ"""
        resolved_path = os.path.abspath(path)
        if not os.path.exists(resolved_path):
            print(f"‚ö†Ô∏è  Warning: Predefined candidate file not found at {resolved_path}. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        try:
            with open(resolved_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as exc:
            print(f"‚ö†Ô∏è  Warning: Failed to load predefined candidate file: {exc}. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        entries = []
        if isinstance(data, dict):
            self.predefined_candidate_config = data.get('config', {})
            entries = data.get('detailed_results') or []
        elif isinstance(data, list):
            entries = data
        else:
            entries = []

        if not isinstance(entries, list) or not entries:
            print("‚ö†Ô∏è  Warning: Predefined candidate file does not contain usable entries. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        # ËßÑËåÉÂåñÂ≠óÊÆµÔºåÁ°Æ‰øùÂÄôÈÄâID‰∏∫Êï¥Êï∞
        normalized_entries: List[Dict[str, Any]] = []
        for entry in entries:
            if not isinstance(entry, dict):
                continue
            original_candidate_ids = entry.get('candidate_ids') or []
            try:
                candidate_ids: List[int] = []
                for item in original_candidate_ids:
                    value = int(item)
                    if value != 0:
                        candidate_ids.append(value)
            except Exception:
                # Â¶ÇÊûúËΩ¨Êç¢Â§±Ë¥•ÂàôË∑≥ËøáËØ•Êù°ÁõÆ
                continue
            normalized_entries.append({
                **entry,
                'candidate_ids': candidate_ids
            })

        if not normalized_entries:
            print("‚ö†Ô∏è  Warning: No valid entries found in predefined candidate file. Falling back to random sampling.")
            self.predefined_candidates_path = None
            return

        self.predefined_candidate_entries = normalized_entries
        total_entries = len(self.predefined_candidate_entries)
        print(f"üìÑ Loaded {total_entries} predefined candidate entries from {resolved_path}")

        # Ê†°È™åÊï∞ÊçÆÈõÜË∑ØÂæÑÊòØÂê¶ÂåπÈÖç
        expected_dataset = self.predefined_candidate_config.get('dataset_path')
        if expected_dataset and os.path.normpath(expected_dataset) != os.path.normpath(self.dataset_dir):
            print(f"‚ö†Ô∏è  Warning: Candidate file dataset ({expected_dataset}) does not match current dataset ({self.dataset_dir})")

    def _build_split_user_index(self, data_split: List[Dict[str, Any]], split_name: str) -> Dict[str, List[int]]:
        """ÁºìÂ≠òÊåáÂÆöÂàíÂàÜ‰∏ãuser_idÂà∞Ê†∑Êú¨Á¥¢ÂºïÁöÑÊò†Â∞ÑÔºåÂä†ÈÄüÊü•Êâæ"""
        cache = self._split_user_index_cache.get(split_name)
        if cache is not None:
            return cache

        mapping: Dict[str, List[int]] = {}
        for idx in range(len(data_split)):
            sample = data_split[idx]
            user_id = sample.get('user_id')
            if user_id is None:
                continue
            mapping.setdefault(user_id, []).append(idx)

        self._split_user_index_cache[split_name] = mapping
        return mapping
    
    def call_api(self, prompt: str, max_retries: int = 5) -> tuple:
        """
        Ë∞ÉÁî®APIÔºå‰ΩøÁî®Êèê‰æõÁöÑÊé•Âè£ÊñπÂºè
        
        Args:
            prompt: ËæìÂÖ•ÊèêÁ§∫
            max_retries: ÊúÄÂ§ßÈáçËØïÊ¨°Êï∞
            
        Returns:
            tuple: (APIÂìçÂ∫îÂÜÖÂÆπ, ÊÄùËÄÉËøáÁ®ã, ÂéüÂßãÂìçÂ∫îÂ≠óÂÖ∏)
        """
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful recommendation system assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=3000
                )
                
                # Ëé∑ÂèñÂìçÂ∫îÂÜÖÂÆπÂíåÊÄùËÄÉËøáÁ®ã
                content = response.choices[0].message.content or ""
                reasoning = getattr(response.choices[0].message, 'reasoning', None) or ""
                
                # ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Áî®‰∫éË∞ÉËØï
                raw_response = response.model_dump() if hasattr(response, 'model_dump') else str(response)
                
                # Â¶ÇÊûúcontent‰∏∫Á©∫ÔºåÊâìÂç∞ÂéüÂßãÂìçÂ∫îÂπ∂ÈáçËØï
                if not content:
                    print(f"‚ö†Ô∏è WARNING: Empty response content received (attempt {attempt + 1}/{max_retries})!")
                    print(f"Raw response: {json.dumps(raw_response, indent=2, ensure_ascii=False) if isinstance(raw_response, dict) else raw_response}")
                    
                    if attempt < max_retries - 1:
                        print(f"üîÑ Retrying due to empty content...")
                        time.sleep(2 ** attempt)  # ÊåáÊï∞ÈÄÄÈÅø
                        continue  # ÈáçËØï
                    else:
                        print(f"‚ùå All {max_retries} attempts returned empty content!")
                        return "", reasoning, raw_response
                
                # content‰∏ç‰∏∫Á©∫ÔºåËøîÂõûÊàêÂäüÁªìÊûú
                return content, reasoning, raw_response
                
            except Exception as e:
                print(f"API call failed (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # ÊåáÊï∞ÈÄÄÈÅø
                else:
                    return "", "", {"error": str(e)}
    
    def create_full_ranking_prompt(self, 
                                 history_titles: List[str], 
                                 all_available_items: List[int],
                                 category: str) -> str:
        """
        ÂàõÂª∫ÂÖ®Â∫ìÂïÜÂìÅÊéíÂ∫èÁöÑÊèêÁ§∫Ôºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÁöÑÂÖ®Â∫ìËØÑ‰º∞ÂÆåÂÖ®‰∏ÄËá¥
        
        Args:
            history_titles: Áî®Êà∑ÂéÜÂè≤ÂïÜÂìÅÊ†áÈ¢ò
            all_available_items: ÊâÄÊúâÂèØÁî®ÂïÜÂìÅIDÔºàÊéíÈô§ÂéÜÂè≤ÂïÜÂìÅÔºâ
            category: ÂïÜÂìÅÁ±ªÂà´
        """
        # Â§ÑÁêÜÂéÜÂè≤Â∫èÂàóÔºå‰∏éËÆ≠ÁªÉÊó∂ÁöÑwindow_size=20‰øùÊåÅ‰∏ÄËá¥
        history_display = history_titles[-20:] if len(history_titles) > 20 else history_titles
        
        # ÊûÑÂª∫ÂéÜÂè≤Â∫èÂàóÂ≠óÁ¨¶‰∏≤
        if len(history_display) == 0:
            history_str = "No previous purchases"
        else:
            history_str = " -> ".join(history_display)
        
        # Áî±‰∫éÂÖ®Â∫ìÂïÜÂìÅÂ§™Â§öÔºåÊàë‰ª¨ÈöèÊú∫ÈááÊ†∑‰∏ÄÈÉ®ÂàÜËøõË°åÊéíÂ∫èÔºàÊ®°ÊãüËÆ≠ÁªÉÊó∂ÁöÑËÆ°ÁÆóÁ∫¶ÊùüÔºâ
        # ‰ΩÜ‰øùËØÅÁõÆÊ†áÂïÜÂìÅ‰∏ÄÂÆöÂú®ÂÖ∂‰∏≠
        sample_size = min(100, len(all_available_items))  # ‰∏éËÆ≠ÁªÉÊó∂ÁöÑbatch sizeÁ±ª‰ºº
        
        prompt = f"""You are a recommendation system for {category} products.

User's purchase history: {history_str}

Based on this history, I need you to rank {sample_size} candidate items by likelihood of purchase.

Please provide your top 20 recommendations with item numbers, ranked from most likely to least likely.
Format your response as a comma-separated list of numbers only.

Example response: 15,3,7,42,88,12,55,73,9,21,34,67,81,96,11,45,78,23,56,39
"""
        
        return prompt
    
    def parse_ranking_response(self, response: str, candidate_items: List[int]) -> List[int]:
        """
        Ëß£ÊûêÊéíÂ∫èÂìçÂ∫îÔºåËøîÂõûÊåâÈ¢ÑÊµãÊ¶ÇÁéáÊéíÂ∫èÁöÑÂïÜÂìÅID
        
        Args:
            response: APIÂìçÂ∫î
            candidate_items: ÂÄôÈÄâÂïÜÂìÅIDÂàóË°®
            
        Returns:
            ÊåâÈ¢ÑÊµãÊ¶ÇÁéáÊéíÂ∫èÁöÑÂïÜÂìÅIDÂàóË°®ÔºàÂè™ËøîÂõûÊ®°ÂûãÈ¢ÑÊµãÁöÑtop-KÔºå‰∏çË°•ÂÖÖÔºâ
        """
        try:
            # ‰ºòÂÖàÊü•Êâæ RANKING: ÂºÄÂ§¥ÁöÑË°å
            ranking_line = None
            lines = response.strip().split('\n')
            
            # ÊñπÊ≥ï1ÔºöÊü•Êâæ RANKING: ÂºÄÂ§¥ÁöÑË°å
            for line in lines:
                if line.strip().startswith('RANKING:'):
                    ranking_line = line.strip()
                    break
            
            if ranking_line:
                # ÊèêÂèñ RANKING: ÂêéÈù¢ÁöÑÊï∞Â≠ó
                ranking_part = ranking_line.split('RANKING:', 1)[1].strip()
                numbers = re.findall(r'\b\d+\b', ranking_part)
                print(f"Found RANKING line: {ranking_part}") if len(candidate_items) <= 100 else None
            else:
                # ÊñπÊ≥ï2ÔºöÊü•ÊâæÊúÄÂêé‰∏ÄË°åÂåÖÂê´ÈÄóÂè∑ÂàÜÈöîÊï∞Â≠óÁöÑË°å
                for line in reversed(lines):
                    line = line.strip()
                    if ',' in line and len(re.findall(r'\b\d+\b', line)) >= 10:
                        numbers = re.findall(r'\b\d+\b', line)
                        print(f"Found comma-separated line: {line}") if len(candidate_items) <= 100 else None
                        break
                else:
                    # ÊñπÊ≥ï3ÔºöÊèêÂèñÊâÄÊúâÊï∞Â≠ó‰Ωú‰∏∫Â§áÈÄâÔºàÊúÄÂêéÁöÑfallbackÔºâ
                    numbers = re.findall(r'\b\d+\b', response)
                    print(f"Fallback: using all numbers from response") if len(candidate_items) <= 100 else None
            
            # Â∞ÜÊï∞Â≠óÊò†Â∞ÑÂõûÂïÜÂìÅIDÔºàÂè™Â§ÑÁêÜÊ®°ÂûãËæìÂá∫ÁöÑÊï∞ÈáèÔºå‰∏çË°•ÂÖÖÔºâ
            predicted_item_ids = []
            invalid_indices = []
            
            for i, num_str in enumerate(numbers):
                try:
                    idx = int(num_str) - 1  # ËΩ¨Êç¢‰∏∫0-indexed
                    if 0 <= idx < len(candidate_items):
                        item_id = candidate_items[idx]
                        if item_id not in predicted_item_ids:
                            predicted_item_ids.append(item_id)
                    else:
                        invalid_indices.append(num_str)
                except:
                    invalid_indices.append(num_str)
            
            if invalid_indices and len(candidate_items) <= 100:
                print(f"Invalid indices found: {invalid_indices[:5]}...")
            
            # ÂÖ≥ÈîÆ‰øÆÊîπÔºö‰∏çÂÜçË°•ÂÖÖÂâ©‰ΩôÂïÜÂìÅÔºÅ
            # Âè™ËøîÂõûÊ®°ÂûãÂÆûÈôÖÈ¢ÑÊµãÁöÑÊéíÂ∫èÔºåÈÄöÂ∏∏ÊòØtop-20
            if len(candidate_items) <= 100:
                print(f"Successfully parsed {len(predicted_item_ids)} items (model prediction), first 10: {[candidate_items.index(id)+1 for id in predicted_item_ids[:10]]}")
            
            return predicted_item_ids
            
        except Exception as e:
            print(f"Error parsing response: {e}")
            # Â¶ÇÊûúËß£ÊûêÂ§±Ë¥•ÔºåËøîÂõûÁ©∫ÂàóË°®ËÄå‰∏çÊòØÈöèÊú∫ÊéíÂ∫è
            return []
    
    def calculate_metrics_like_training(self, predicted_ranking: List[int], target_id: int) -> Dict[str, float]:
        """
        ËÆ°ÁÆóËØÑ‰º∞ÊåáÊ†áÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÂÆåÂÖ®‰∏ÄËá¥
        ÈááÁî®‰∏étrainers/utils.py‰∏≠calculate_metricsÂáΩÊï∞Áõ∏ÂêåÁöÑÈÄªËæë
        
        Args:
            predicted_ranking: È¢ÑÊµãÁöÑÂïÜÂìÅIDÊéíÂ∫èÔºàÂèØËÉΩÂè™Êúâtop-K‰∏™ÔºåÂ¶Ç20‰∏™Ôºâ
            target_id: ÁõÆÊ†áÂïÜÂìÅID
            
        Returns:
            ËØÑ‰º∞ÊåáÊ†áÂ≠óÂÖ∏
        """
        metrics = {}
        
        # ÊâæÂà∞ÁõÆÊ†áÂïÜÂìÅÂú®È¢ÑÊµãÊéíÂ∫è‰∏≠ÁöÑ‰ΩçÁΩÆÔºà0-indexedÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥Ôºâ
        try:
            target_position = predicted_ranking.index(target_id)  # 0-indexed position
        except ValueError:
            # ÁõÆÊ†áÂïÜÂìÅ‰∏çÂú®È¢ÑÊµã‰∏≠ÔºåËÆ§‰∏∫ÊéíÂú®È¢ÑÊµãÂàóË°®‰πãÂêé
            # ÂØπ‰∫éNDCG@KËÆ°ÁÆóÔºåÂ¶ÇÊûúÁõÆÊ†á‰∏çÂú®top-K‰∏≠ÔºåÂàôË¥°ÁåÆ‰∏∫0
            target_position = len(predicted_ranking)  # ËÆæ‰∏∫È¢ÑÊµãÂàóË°®ÈïøÂ∫¶ÔºåË°®Á§∫‰∏çÂú®È¢ÑÊµã‰∏≠
        
        # ËÆ°ÁÆóHit Rate @ KÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÁöÑks=[5, 10, 20]‰∏ÄËá¥
        for k in [5, 10, 20]:
            # Â¶ÇÊûúÁõÆÊ†áÂú®top-k‰∏≠ÔºåÂàôÂëΩ‰∏≠
            metrics[f'hit_rate@{k}'] = 1.0 if target_position < k else 0.0
        
        # ËÆ°ÁÆóDCG @ KÔºå‰ΩøÁî®‰∏éËÆ≠ÁªÉËÑöÊú¨ÂÆåÂÖ®Áõ∏ÂêåÁöÑÂÖ¨Âºè
        # ËÆ≠ÁªÉËÑöÊú¨Ôºödiscount = torch.log2(torch.arange(2, cutoff + 2))
        # dcg = (1.0 / discount)
        # Ê≥®ÊÑèÔºöposition‰ªé0ÂºÄÂßãÔºåÊâÄ‰ª•discountÁöÑÁ¥¢ÂºïÊòØposition+2
        for k in [5, 10, 20]:
            if target_position < k:
                # ‰∏éËÆ≠ÁªÉËÑöÊú¨ÂÆåÂÖ®‰∏ÄËá¥Ôºöposition 0ÂØπÂ∫îlog2(2), position 1ÂØπÂ∫îlog2(3)...
                dcg_value = 1.0 / np.log2(target_position + 2)
                metrics[f'ndcg@{k}'] = dcg_value  # ËÆ≠ÁªÉËÑöÊú¨Ê≤°ÊúâIDCGÂΩí‰∏ÄÂåñÔºåÁõ¥Êé•‰ΩøÁî®DCG
            else:
                # ÁõÆÊ†áÂïÜÂìÅ‰∏çÂú®top-k‰∏≠ÔºåË¥°ÁåÆ‰∏∫0
                metrics[f'ndcg@{k}'] = 0.0
        
        return metrics
    
    def evaluate_like_training(self, 
                             num_samples: Optional[int] = None,  # Êîπ‰∏∫ÂèØÈÄâÔºåÈªòËÆ§ÊµãËØïÂÖ®ÈõÜ
                             split: str = 'test',
                             save_results: bool = True,
                             progress_interval: int = 100,  # Êñ∞Â¢ûÔºöËøõÂ∫¶ËæìÂá∫Èó¥Èöî
                             save_interval: int = 50,  # Êñ∞Â¢ûÔºö‰øùÂ≠òÈó¥Èöî
                             preserve_order: bool = False) -> Dict[str, float]:
        """
        ËØÑ‰º∞ÂáΩÊï∞ÔºåÂÆåÂÖ®Ê®°ÊãüËÆ≠ÁªÉËÑöÊú¨‰∏≠ÁöÑËØÑ‰º∞ËøáÁ®ã
        
        Args:
            num_samples: ÊµãËØïÊ†∑Êú¨Êï∞ÈáèÔºåNoneË°®Á§∫ÊµãËØïÂÖ®ÈõÜÔºà‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥Ôºâ
            split: Êï∞ÊçÆÈõÜÂàÜÂâ≤ ('test' Êàñ 'valid')
            save_results: ÊòØÂê¶‰øùÂ≠òËØ¶ÁªÜÁªìÊûú
            progress_interval: ÊØèÈöîÂ§öÂ∞ë‰∏™Ê†∑Êú¨ËæìÂá∫‰∏ÄÊ¨°ËøõÂ∫¶ÔºàÈªòËÆ§100Ôºâ
            save_interval: ÊØèÈöîÂ§öÂ∞ë‰∏™Ê†∑Êú¨‰øùÂ≠ò‰∏ÄÊ¨°‰∏≠Èó¥ÁªìÊûúÔºàÈªòËÆ§50Ôºâ
            
        Returns:
            ËØÑ‰º∞ÊåáÊ†áÂ≠óÂÖ∏
        """
        if self.seed is not None:
            random.seed(self.seed)
            np.random.seed(self.seed)

        data = self.test_data if split == 'test' else self.valid_data
        total_samples = len(data)
        using_predefined = bool(self.predefined_candidate_entries) and split == self.predefined_candidate_split
        if using_predefined:
            predefined_total = len(self.predefined_candidate_entries)
            if num_samples is None or num_samples > predefined_total:
                num_samples = predefined_total
            print(f"‚ÑπÔ∏è  Using predefined candidate JSON for {split} split with {predefined_total} available entries")
        
        # Â¶ÇÊûúnum_samples‰∏∫NoneÔºåÊµãËØïÂÖ®ÈõÜÔºà‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥Ôºâ
        if num_samples is None:
            num_samples = total_samples
            print(f"\n=== Evaluating API like Training Script on {split} set (FULL DATASET: {total_samples} samples) ===")
        else:
            num_samples = min(num_samples, total_samples)
            print(f"\n=== Evaluating API like Training Script on {split} set ({num_samples}/{total_samples} samples, preserve_order={preserve_order}) ===")
        
        print(f"üíæ Incremental save every {save_interval} samples")
        category = self.dataset_dir.split('/')[-1].split('_')[0].replace('_', ' ')

        prepared_samples = self.prepare_rank_candidates(data, num_samples, split, preserve_order=preserve_order)
        if not prepared_samples:
            print(f"‚ùå No valid user samples available for {split} evaluation")
            return {}

        total_to_evaluate = len(prepared_samples)
        candidate_count = len(prepared_samples[0]['candidate_items']) if prepared_samples else 0
        
        # ÂàùÂßãÂåñÁ¥ØÁßØÊåáÊ†áÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÁöÑMetricUpdater‰∏ÄËá¥
        accumulated_metrics = {
            'hit_rate@5': 0.0, 'hit_rate@10': 0.0, 'hit_rate@20': 0.0,
            'ndcg@5': 0.0, 'ndcg@10': 0.0, 'ndcg@20': 0.0
        }
        
        valid_samples = 0
        detailed_results = []
        prompt_response_logs = []  # Êñ∞Â¢ûÔºö‰øùÂ≠òpromptÂíåresponseÁî®‰∫éË∞ÉËØï
        
        for prepared in tqdm(prepared_samples, desc=f"Evaluating {split}"):
            sample = prepared['sample']
            history_ids = prepared['history_ids']
            history_titles = prepared['history_titles']
            target_id = prepared['target_id']
            target_title = prepared['target_title']
            candidate_items = prepared['candidate_items']
            sample_index = prepared['sample_index']
            user_id = prepared['user_id']
            target_candidate_position = prepared.get('target_candidate_position')
            
            # ÊûÑÂª∫ÂÄôÈÄâÂïÜÂìÅÊ†áÈ¢òÂàóË°®Áî®‰∫éprompt
            candidate_titles = [self.id2title[item_id] for item_id in candidate_items]
            candidates_str = "\n".join([f"{i+1}. {title}" for i, title in enumerate(candidate_titles)])
            
            # ÂàõÂª∫promptÔºåÊòéÁ°ÆÊåáÂÆöËæìÂá∫Ê†ºÂºè
            prompt = f"""You are a recommendation system for {category} products.

User's purchase history: {" -> ".join(history_titles[-20:])}

Please rank the following {len(candidate_items)} items by likelihood of purchase.

Candidate items:
{candidates_str}

IMPORTANT: Your response must end with exactly one line in this format:
RANKING: number1,number2,number3,number4,number5,number6,number7,number8,number9,number10,number11,number12,number13,number14,number15,number16,number17,number18,number19,number20

Where each number is between 1-{len(candidate_items)}. Example:
RANKING: 26,45,78,50,38,99,77,43,53,89,8,93,97,52,47,31,48,83,98,79
"""
            
            # Ë∞ÉÁî®API
            response, reasoning, raw_response = self.call_api(prompt)
            
            # Ëß£ÊûêÂìçÂ∫îËé∑ÂæóÊéíÂ∫è
            predicted_ranking = self.parse_ranking_response(response, candidate_items)
            
            # Êõ¥Á≤æÁ°ÆÁöÑËß£ÊûêÊàêÂäüÁéáÊ£ÄÊü•
            # parsing_success Áé∞Âú®Ë°®Á§∫ÊòØÂê¶ÊàêÂäüËß£ÊûêÂá∫‰∫ÜÊé®ËçêÂàóË°®Ôºà‰∏çË¶ÅÊ±ÇÂÆåÊï¥Ôºâ
            parsing_success = (len(predicted_ranking) > 0)
            
            # ËÆ°ÁÆóÁõÆÊ†áÂïÜÂìÅÂú®È¢ÑÊµã‰∏≠ÁöÑÊéíÂêçÔºà1-indexed for displayÔºâ
            if target_id in predicted_ranking:
                target_rank_in_prediction = predicted_ranking.index(target_id) + 1
            else:
                # ÁõÆÊ†á‰∏çÂú®È¢ÑÊµãÁöÑtop-K‰∏≠ÔºåËÆæ‰∏∫È¢ÑÊµãÈïøÂ∫¶+1ÔºàË°®Á§∫ÊéíÂú®È¢ÑÊµãÂàóË°®Â§ñÔºâ
                target_rank_in_prediction = len(predicted_ranking) + 1
            
            # ËÆ°ÁÆóÊåáÊ†á
            sample_metrics = self.calculate_metrics_like_training(predicted_ranking, target_id)
            
            # Á¥ØÁßØÊåáÊ†á
            for key in accumulated_metrics:
                accumulated_metrics[key] += sample_metrics[key]
            
            valid_samples += 1
            
            # ‰øùÂ≠òËØ¶ÁªÜÁªìÊûú
            if save_results:
                detailed_results.append({
                    'sample_id': sample_index,
                    'user_id': user_id,
                    'history_titles': history_titles[-10:],
                    'target_title': target_title,
                    'target_rank': target_rank_in_prediction,
                    'predicted_items_count': len(predicted_ranking),  # Êñ∞Â¢ûÔºöÂÆûÈôÖÈ¢ÑÊµãÁöÑÂïÜÂìÅÊï∞Èáè
                    'total_candidates': len(candidate_items),
                    'parsing_success': parsing_success,
                    'target_candidate_position': target_candidate_position,
                    'metrics': sample_metrics,
                    'api_response': response[:1000],  # Êà™Êñ≠‰øùÂ≠ò
                    'reasoning_preview': reasoning[:1000] if reasoning else None  # Êñ∞Â¢ûÔºöÊÄùËÄÉËøáÁ®ãÈ¢ÑËßà
                })
                
                # ‰øùÂ≠òËØ¶ÁªÜÁöÑpromptÂíåresponseÁî®‰∫éË∞ÉËØï
                prompt_response_logs.append({
                    'sample_id': sample_index,
                    'user_id': user_id,
                    'prompt': prompt,
                    'response': response,
                    'reasoning': reasoning,  # Êñ∞Â¢ûÔºöÂÆåÊï¥ÁöÑÊÄùËÄÉËøáÁ®ã
                    'raw_response': raw_response,  # Êñ∞Â¢ûÔºöÂéüÂßãÂìçÂ∫îÁî®‰∫éË∞ÉËØï
                    'candidate_items': candidate_items,
                    'predicted_ranking': predicted_ranking,
                    'target_id': target_id,
                    'target_rank': target_rank_in_prediction,
                    'target_candidate_position': target_candidate_position,
                    'parsing_success': parsing_success,
                    'sample_metrics': sample_metrics
                })
            
            # ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫ÔºöÊØèprogress_interval‰∏™Ê†∑Êú¨ËæìÂá∫‰∏ÄÊ¨°‰∏≠Èó¥ÁªìÊûú
            if valid_samples % progress_interval == 0:
                current_metrics = {k: v / valid_samples for k, v in accumulated_metrics.items()}
                print(f"\n--- Progress Update: {valid_samples}/{total_to_evaluate} samples ---")
                print(f"Current NDCG@10: {current_metrics['ndcg@10']:.6f}")
                print(f"Current Hit Rate@10: {current_metrics['hit_rate@10']:.6f}")
                print(f"Target found in top-10: {sum(1 for r in detailed_results[-progress_interval:] if r['target_rank'] <= 10)}/{min(progress_interval, len(detailed_results))} samples")
                print(f"Parsing success rate: {sum(1 for r in detailed_results[-progress_interval:] if r['parsing_success'])}/{min(progress_interval, len(detailed_results))} samples")
                print("-" * 50)
            
            # Â¢ûÈáè‰øùÂ≠òÔºöÊØèsave_interval‰∏™Ê†∑Êú¨‰øùÂ≠ò‰∏ÄÊ¨°‰∏≠Èó¥ÁªìÊûú
            if save_results and valid_samples % save_interval == 0:
                self._save_incremental_results(
                    accumulated_metrics, detailed_results, prompt_response_logs,
                    valid_samples, split, total_to_evaluate, progress_interval,
                    preserve_order
                )
        
        # ËÆ°ÁÆóÂπ≥ÂùáÊåáÊ†áÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨ÁöÑMetricUpdater.compute()‰∏ÄËá¥
        if valid_samples > 0:
            for key in accumulated_metrics:
                accumulated_metrics[key] = accumulated_metrics[key] / valid_samples
        
        accumulated_metrics['total_samples'] = valid_samples
        
        # ÊâìÂç∞ÊúÄÁªàÁªìÊûúÔºåÊ†ºÂºè‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥
        print(f"\n{'='*60}")
        print(f"FINAL EVALUATION RESULTS ({valid_samples} samples)")
        print(f"{'='*60}")
        print(f"hit_rate@5:  {accumulated_metrics['hit_rate@5']:.6f}")
        print(f"hit_rate@10: {accumulated_metrics['hit_rate@10']:.6f}")
        print(f"hit_rate@20: {accumulated_metrics['hit_rate@20']:.6f}")
        print(f"ndcg@5:      {accumulated_metrics['ndcg@5']:.6f}")
        print(f"ndcg@10:     {accumulated_metrics['ndcg@10']:.6f}")  # ‰∏éËÆ≠ÁªÉÊó•ÂøóÊ†ºÂºè‰∏ÄËá¥
        print(f"ndcg@20:     {accumulated_metrics['ndcg@20']:.6f}")
        
        # ËÆ°ÁÆóÂíåÊòæÁ§∫È¢ùÂ§ñÁöÑÂàÜÊûêÊåáÊ†á
        if detailed_results:
            target_in_top10_count = sum(1 for r in detailed_results if r['target_rank'] <= 10)
            parsing_success_count = sum(1 for r in detailed_results if r['parsing_success'])
            avg_predicted_items = sum(r['predicted_items_count'] for r in detailed_results) / len(detailed_results)
            print(f"\nAdditional Analysis:")
            print(f"Target in Top-10: {target_in_top10_count}/{len(detailed_results)} ({target_in_top10_count/len(detailed_results)*100:.1f}%)")
            print(f"Parsing Success: {parsing_success_count}/{len(detailed_results)} ({parsing_success_count/len(detailed_results)*100:.1f}%)")
            print(f"Average Predicted Items: {avg_predicted_items:.1f} (vs {candidate_count} candidates)")
            target_positions = [r['target_candidate_position'] for r in detailed_results if r.get('target_candidate_position')]
            if target_positions:
                avg_target_position = sum(target_positions) / len(target_positions)
                print(f"Average Target Position in Candidates: {avg_target_position:.1f} ({self.target_position_mode})")
            
            # ÊòæÁ§∫‰∏Ä‰∫õÁ§∫‰æã
            print(f"\nExample predictions (first 3 samples):")
            for i, result in enumerate(detailed_results[:3]):
                print(f"  Sample {result['sample_id']}: Target rank {result['target_rank']}/{result['predicted_items_count']} predicted, NDCG@10={result['metrics']['ndcg@10']:.4f}")
        
        # ‰øùÂ≠òÁªìÊûú
        if save_results:
            # ‰øùÂ≠ò‰∏ªË¶ÅÁªìÊûú
            results_file = os.path.join(
                self.output_dir,
                f"deepseek_api_results_{split}_{self.model_name.replace('/', '_')}.json"
            )
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'metrics': accumulated_metrics,
                    'detailed_results': detailed_results,
                    'config': {
                        'dataset_dir': self.dataset_dir,
                        'model_name': self.model_name,
                        'num_samples': total_to_evaluate,
                        'split': split,
                        'evaluation_method': 'training_consistent',
                        'api_base': self.api_base,
                        'progress_interval': progress_interval,
                        'seed': self.seed,
                        'target_position_mode': self.target_position_mode,
                        'output_dir': self.output_dir,
                        'test_sample_limit': self.test_limit,
                        'preserve_order': preserve_order
                    }
                }, f, indent=2, ensure_ascii=False)
            print(f"\nDetailed results saved to {results_file}")
            
            # ‰øùÂ≠òpromptÂíåresponseÊó•ÂøóÁî®‰∫éË∞ÉËØï
            if prompt_response_logs:
                debug_log_file = os.path.join(
                    self.output_dir,
                    f"api_debug_logs_{split}_{self.model_name.replace('/', '_')}.json"
                )
                with open(debug_log_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'logs': prompt_response_logs,
                        'summary': {
                            'total_samples': len(prompt_response_logs),
                            'parsing_success_rate': sum(1 for log in prompt_response_logs if log['parsing_success']) / len(prompt_response_logs),
                            'avg_target_rank': sum(log['target_rank'] for log in prompt_response_logs) / len(prompt_response_logs),
                            'config': {
                                'dataset_dir': self.dataset_dir,
                                'model_name': self.model_name,
                                'split': split,
                                'target_position_mode': self.target_position_mode,
                                'output_dir': self.output_dir,
                                'test_sample_limit': self.test_limit,
                                'preserve_order': preserve_order
                            }
                        }
                    }, f, indent=2, ensure_ascii=False)
                print(f"Debug logs (prompts & responses) saved to {debug_log_file}")
        
        return accumulated_metrics
    
    def _save_incremental_results(self, accumulated_metrics, detailed_results, prompt_response_logs,
                                 valid_samples, split, num_samples, progress_interval,
                                 preserve_order: bool = False):
        """Â¢ûÈáè‰øùÂ≠ò‰∏≠Èó¥ÁªìÊûúÔºåÈò≤Ê≠¢Á®ãÂ∫è‰∏≠Êñ≠ÂØºËá¥Êï∞ÊçÆ‰∏¢Â§±"""
        try:
            # ËÆ°ÁÆóÂΩìÂâçÂπ≥ÂùáÊåáÊ†á
            current_metrics = {k: v / valid_samples for k, v in accumulated_metrics.items()}
            current_metrics['total_samples'] = valid_samples
            
            # ‰øùÂ≠ò‰∏≠Èó¥ÁªìÊûúÊñá‰ª∂ÔºàÂ∏¶Êó∂Èó¥Êà≥Ôºâ
            import time
            timestamp = int(time.time())
            
            # ‰∏ªË¶ÅÁªìÊûúÁöÑÂ¢ûÈáè‰øùÂ≠ò
            incremental_file = os.path.join(
                self.output_dir,
                f"api_incremental_{split}_{self.model_name.replace('/', '_')}_{valid_samples}samples_{timestamp}.json"
            )
            with open(incremental_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'current_metrics': current_metrics,
                    'detailed_results': detailed_results,
                    'status': {
                        'completed_samples': valid_samples,
                        'total_samples': num_samples,
                        'progress_percentage': (valid_samples / num_samples * 100) if num_samples else 0,
                        'timestamp': timestamp,
                        'is_incremental': True
                    },
                    'config': {
                        'dataset_dir': self.dataset_dir,
                        'model_name': self.model_name,
                        'split': split,
                        'evaluation_method': 'training_consistent',
                        'api_base': self.api_base,
                        'progress_interval': progress_interval,
                        'seed': self.seed,
                        'target_position_mode': self.target_position_mode,
                        'output_dir': self.output_dir,
                        'test_sample_limit': self.test_limit,
                        'preserve_order': preserve_order
                    }
                }, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Incremental save: {incremental_file} ({valid_samples} samples)")
            
            # ‰øùÂ≠òÊúÄÊñ∞ÁöÑË∞ÉËØïÊó•ÂøóÔºàË¶ÜÁõñÊ®°ÂºèÔºâ
            if prompt_response_logs:
                debug_incremental_file = os.path.join(
                    self.output_dir,
                    f"api_debug_incremental_{split}_{self.model_name.replace('/', '_')}.json"
                )
                with open(debug_incremental_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'logs': prompt_response_logs,
                        'summary': {
                            'total_samples': len(prompt_response_logs),
                            'parsing_success_rate': sum(1 for log in prompt_response_logs if log['parsing_success']) / len(prompt_response_logs),
                            'avg_target_rank': sum(log['target_rank'] for log in prompt_response_logs) / len(prompt_response_logs),
                            'timestamp': timestamp,
                            'is_incremental': True,
                            'target_position_mode': self.target_position_mode,
                            'output_dir': self.output_dir,
                            'test_sample_limit': self.test_limit,
                            'preserve_order': preserve_order
                        }
                    }, f, indent=2, ensure_ascii=False)
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Failed to save incremental results: {e}")
    
    def run_full_evaluation(self, 
                          test_samples: Optional[int] = None,  # Êîπ‰∏∫ÂèØÈÄâÔºåÈªòËÆ§ÂÖ®ÊµãËØïÈõÜ
                          valid_samples: Optional[int] = None,  # Êîπ‰∏∫ÂèØÈÄâÔºåÈªòËÆ§ÂÖ®È™åËØÅÈõÜ
                          progress_interval: int = 100,  # Ê∑ªÂä†ËøõÂ∫¶Èó¥ÈöîÂèÇÊï∞
                          save_interval: int = 50,  # Êñ∞Â¢ûÔºö‰øùÂ≠òÈó¥ÈöîÂèÇÊï∞
                          test_only: bool = False):  # Êñ∞Â¢ûÔºö‰ªÖÊµãËØïÊµãËØïÈõÜ
        """ËøêË°åÂÆåÊï¥ËØÑ‰º∞ÔºåÂú®È™åËØÅÈõÜÂíåÊµãËØïÈõÜ‰∏äÈÉΩËøõË°åÊµãËØïÔºå‰∏éËÆ≠ÁªÉËÑöÊú¨‰∏ÄËá¥"""
        print("=" * 60)
        print(f"API Recommendation System Evaluation (Training Consistent)")
        print(f"Dataset: {self.dataset_dir}")
        print(f"Model: {self.model_name}")
        print(f"API Base: {self.api_base}")
        print(f"Evaluation Method: Full item library ranking (like training)")
        print(f"Progress interval: Every {progress_interval} samples")
        print(f"Save interval: Every {save_interval} samples")
        print(f"Target position mode: {self.target_position_mode}")
        print(f"Output directory: {self.output_dir}")
        if self.test_limit is not None:
            print(f"Test sample limit (first-N in order when applicable): {self.test_limit}")
        if test_only:
            print(f"üí∞ Cost-saving mode: Testing ONLY the test set")
        print("=" * 60)
        
        self.test_only_mode = test_only
        preserve_order_for_test = test_only and self.test_limit is not None

        valid_metrics = None
        
        # È™åËØÅÈõÜËØÑ‰º∞Ôºà‰ªÖÂú®Èùûtest_onlyÊ®°Âºè‰∏ãÔºâ
        if not test_only:
            print("\n1. Validation Set Evaluation")
            valid_metrics = self.evaluate_like_training(
                num_samples=valid_samples,  # NoneË°®Á§∫ÂÖ®ÈõÜ
                split='valid',
                progress_interval=progress_interval,  # ‰ΩøÁî®‰º†ÂÖ•ÁöÑËøõÂ∫¶Èó¥Èöî
                save_interval=save_interval,  # ‰ΩøÁî®‰º†ÂÖ•ÁöÑ‰øùÂ≠òÈó¥Èöî
                preserve_order=False
            )
        else:
            print("\n‚è≠Ô∏è  Skipping validation set evaluation (test_only mode)")
        
        # ÊµãËØïÈõÜËØÑ‰º∞
        eval_number = "1" if test_only else "2"
        print(f"\n{eval_number}. Test Set Evaluation")
        requested_test_samples = self.test_limit if preserve_order_for_test else test_samples
        test_metrics = self.evaluate_like_training(
            num_samples=requested_test_samples,  # NoneË°®Á§∫ÂÖ®ÈõÜ
            split='test',
            progress_interval=progress_interval,  # ‰ΩøÁî®‰º†ÂÖ•ÁöÑËøõÂ∫¶Èó¥Èöî
            save_interval=save_interval,  # ‰ΩøÁî®‰º†ÂÖ•ÁöÑ‰øùÂ≠òÈó¥Èöî
            preserve_order=preserve_order_for_test
        )
        
        # Ê±áÊÄªÁªìÊûú
        print("\n" + "=" * 60)
        print("FINAL RESULTS SUMMARY (Training Consistent)")
        print("=" * 60)
        
        if valid_metrics:
            print("\nValidation Set:")
            for metric, value in valid_metrics.items():
                if metric != 'total_samples':
                    print(f"  {metric}: {value:.4f}")
        
        print("\nTest Set:")
        for metric, value in test_metrics.items():
            if metric != 'total_samples':
                print(f"  {metric}: {value:.4f}")
        
        # ‰∏éËÆ≠ÁªÉÊó•ÂøóÁöÑÁõÆÊ†áÊåáÊ†áÂØπÊØî
        train_target_ndcg = 0.012233516966979588
        test_ndcg = test_metrics.get('ndcg@10', 0)
        print(f"\nüìä Comparison with training target:")
        print(f"  Training model ndcg@10: {train_target_ndcg:.6f}")
        print(f"  {self.model_name} ndcg@10:  {test_ndcg:.6f}")
        print(f"  Difference: {test_ndcg - train_target_ndcg:+.6f}")
        
        # ‰øùÂ≠òÊ±áÊÄªÁªìÊûú
        summary_results = {
            'test': test_metrics,
            'config': {
                'dataset_dir': self.dataset_dir,
                'model_name': self.model_name,
                'api_base': self.api_base,
                'test_samples_requested': requested_test_samples,
                'test_samples_evaluated': test_metrics.get('total_samples'),
                'evaluation_method': 'training_consistent',
                'test_only_mode': test_only,
                'seed': self.seed,
                'target_position_mode': self.target_position_mode,
                'output_dir': self.output_dir,
                'test_sample_limit': self.test_limit,
                'preserve_order_for_test': preserve_order_for_test
            }
        }
        
        if valid_metrics:
            summary_results['validation'] = valid_metrics
            summary_results['config']['valid_samples_requested'] = valid_samples
            summary_results['config']['valid_samples_evaluated'] = valid_metrics.get('total_samples')
        
        summary_file = os.path.join(
            self.output_dir,
            f"api_consistent_summary_{self.model_name.replace('/', '_')}.json"
        )
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_results, f, indent=2, ensure_ascii=False)
        print(f"\nSummary results saved to {summary_file}")
        
        if test_only:
            return test_metrics
        else:
            return valid_metrics, test_metrics

def main():
    parser = argparse.ArgumentParser(description="Test API with training-consistent evaluation")
    parser.add_argument("--api_key", type=str, default="sk-ZUK05LeE0U3FVQAp6e5eCd3628774c4181D513786bD3B901", help="API key")
    parser.add_argument("--dataset_dir", type=str, default="/data/hongdeyao/Movies_and_TV_0_2022-10-2023-10", help="Dataset directory")
    parser.add_argument("--test_samples", type=int, default=200, help="Number of test samples (None for full dataset)")
    parser.add_argument("--test_limit", type=int, default=None,
                        help="In test-only mode, evaluate only the first N samples from the test split")
    parser.add_argument("--valid_samples", type=int, default=None, help="Number of validation samples (None for full dataset)")
    parser.add_argument("--api_base", type=str, default="http://115.182.62.174:18888/v1", help="API base URL")
    parser.add_argument("--model_name", type=str, default="deepseek/deepseek-r1", 
                       choices=["deepseek/deepseek-r1"],
                       help="Model name")
    parser.add_argument("--sample_for_api_efficiency", action="store_true", 
                       help="Use sampling for API efficiency (default: False, test full dataset like training)")
    parser.add_argument("--progress_interval", type=int, default=2, 
                       help="Show progress every N samples (default: 50)")
    parser.add_argument("--save_interval", type=int, default=5, 
                       help="Save incremental results every N samples (default: 25)")
    parser.add_argument("--test_only", action="store_true", 
                       help="Only evaluate test set (saves API costs)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for candidate sampling")
    parser.add_argument("--target_position", type=str, default="random",
                        choices=["first", "last", "random"],
                        help="Control where the correct item appears in candidates (default: first)")
    parser.add_argument("--output_dir", type=str, default="/data/hongdeyao/code/RRec/sft/outputs/movies_200_sample",
                        help="Directory to save evaluation outputs (default: current directory)")
    parser.add_argument("--candidate_mode", type=str, default="json",
                        choices=["random", "json"],
                        help="Candidate selection mode: random sampling or predefined JSON")
    parser.add_argument("--candidate_json_path", type=str, default="/data/hongdeyao/code/RRec/sft/outputs/movies_200_sample/checkpoint-009000.json",
                        help="Path to predefined candidate JSON file when candidate_mode=json")
    parser.add_argument("--candidate_json_split", type=str, default="test",
                        choices=["train", "valid", "test"],
                        help="Dataset split to pair with the predefined candidate JSON entries")
    
    args = parser.parse_args()
    
    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê‰ª•Á°Æ‰øùÂèØÂ§çÁé∞ÊÄß
    random.seed(args.seed)
    np.random.seed(args.seed)
    
    # Â¶ÇÊûúÂêØÁî®ÈááÊ†∑Ê®°ÂºèÔºåËÆæÁΩÆÈªòËÆ§Ê†∑Êú¨Êï∞
    if args.sample_for_api_efficiency:
        test_samples = args.test_samples or 1000
        valid_samples = args.valid_samples or 500
        print("‚ö†Ô∏è  API efficiency mode enabled: using sampling instead of full dataset")
    else:
        test_samples = args.test_samples  # NoneË°®Á§∫ÂÖ®ÈõÜ
        valid_samples = args.valid_samples  # NoneË°®Á§∫ÂÖ®ÈõÜ
        print("‚úÖ Full dataset evaluation mode (same as training script)")

    if args.test_limit is not None and not args.test_only:
        print("‚ÑπÔ∏è  test_limit is set but test_only mode is disabled; limit will apply only when test_only is True.")

    if args.candidate_mode == "json":
        candidate_json_path = os.path.abspath(args.candidate_json_path)
        print(f"üìÑ Candidate selection mode: JSON (path={candidate_json_path})")
    else:
        candidate_json_path = None
        print("üé≤ Candidate selection mode: random sampling from dataset")
    
    # ÂàùÂßãÂåñÊµãËØïÂô®
    tester = APIRecommendationTester(
        api_key=args.api_key,
        dataset_dir=args.dataset_dir,
        api_base=args.api_base,
        model_name=args.model_name,
        seed=args.seed,
        target_position=args.target_position,
        output_dir=args.output_dir,
        test_limit=args.test_limit,
        predefined_candidates_path=candidate_json_path,
        predefined_candidate_split=args.candidate_json_split
    )
    
    # ËøêË°åÂÆåÊï¥ËØÑ‰º∞
    tester.run_full_evaluation(
        test_samples=test_samples,
        valid_samples=valid_samples,
        progress_interval=args.progress_interval,
        save_interval=args.save_interval,
        test_only=args.test_only
    )

if __name__ == "__main__":
    main()
