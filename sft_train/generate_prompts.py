#!/usr/bin/env python3
"""
ç”Ÿæˆå¯é…ç½®åŒ¹é…/ä¸åŒ¹é…æ¯”ä¾‹çš„äºŒåˆ†ç±»prompté—®é¢˜

ä¸»è¦åŠŸèƒ½ï¼š
1. ä»Musical_Instrumentsæ•°æ®é›†éšæœºæŠ½å–nä¸ªæ ·æœ¬
2. å¯é…ç½®å¤šå°‘ä¸ªpromptä½¿ç”¨åŒ¹é…çš„å€™é€‰ç‰©å“ï¼Œå¤šå°‘ä¸ªä½¿ç”¨ä¸åŒ¹é…çš„å€™é€‰ç‰©å“
3. ä½¿ç”¨å›ºå®šéšæœºç§å­42ç¡®ä¿ç»“æœå¯å¤ç°

ä½¿ç”¨æ–¹æ³•ç¤ºä¾‹ï¼š
python generate_prompts.py --num_samples 100 --match_ratio 0.5 --dataset_dir /data/hongdeyao/Musical_Instruments_0_2022-10-2023-10
"""

import os
import sys
import json
import random
import numpy as np
from typing import List, Dict, Any
from datasets import load_from_disk
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/data/hongdeyao/code/RRec')

from paths import model_names
from trainers.utils import get_tokenizer
from prompters.rrec_prompter import UserGenPrompter


class PromptGenerator:
    def __init__(self, 
                 dataset_dir: str,
                 model_type: str = "qwen",
                 seed: int = 42):
        """
        åˆå§‹åŒ–promptç”Ÿæˆå™¨
        """
        self.dataset_dir = dataset_dir
        self.model_type = model_type
        self.seed = seed
        
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        np.random.seed(seed)
        
        print(f"ğŸš€ Initializing Prompt Generator")
        print(f"ğŸ“Š Dataset: {dataset_dir}")
        print(f"ğŸ¤– Model: {model_type}")
        print(f"ğŸ² Seed: {seed}")

        # åŠ è½½æ•°æ®é›†
        self.load_dataset()
        
        # åŠ è½½tokenizer
        self.load_tokenizer()
        
        # åˆå§‹åŒ–prompter
        self.init_prompter()

    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        print(f"ğŸ“‚ Loading dataset from {self.dataset_dir}")
        try:
            self.dataset = load_from_disk(self.dataset_dir)
            self.test_data = self.dataset['valid']
            self.item_info = self.dataset['item_info']
            # åˆ›å»ºitemæ˜ å°„
            self.create_item_mappings()
            print(f"âœ… Dataset loaded successfully:")
            print(f"  Test samples: {len(self.test_data)}")
            print(f"  Items: {len(self.item_info)}")
        except Exception as e:
            print(f"âŒ Error loading dataset: {e}")
            raise

    def create_item_mappings(self):
        """åˆ›å»ºç‰©å“IDåˆ°æ ‡é¢˜çš„æ˜ å°„"""
        self.id2title = {}
        self.all_item_ids = []
        self.item_dict = {}
        
        for item in self.item_info:
            item_id = item['item_id']
            title = item.get('title', f'Unknown Item {item_id}')
            self.id2title[item_id] = title
            self.item_dict[item_id] = item
            if item_id != 0:  # æ’é™¤paddingé¡¹
                self.all_item_ids.append(item_id)
        
        print(f"ğŸ“‹ Created mappings for {len(self.id2title)} items (including {len(self.all_item_ids)} non-pad items)")

    def load_tokenizer(self):
        """åŠ è½½tokenizer"""
        print(f"ğŸ”„ Loading tokenizer for model family: {self.model_type}")
        try:
            if self.model_type == 'qwen':
                model_name = model_names["Qwen2.5-3B-Instruct"]
            elif self.model_type == 'gemma':
                model_name = model_names["Gemma-2-2b-it"]
            else:
                model_name = list(model_names.values())[0]

            self.tokenizer = get_tokenizer(model_name)
            print("âœ… Tokenizer loaded")
        except Exception as e:
            print(f"âš ï¸ Warning: failed to load tokenizer: {e}. Some prompt templating features may be limited.")
            self.tokenizer = None

    def init_prompter(self):
        """åˆå§‹åŒ–prompter"""
        print("ğŸ”„ Initializing prompter...")
        
        # æ ¹æ®æ•°æ®é›†è·¯å¾„ç¡®å®šç±»åˆ«
        dataset_name = self.dataset_dir.split('/')[-1]
        if 'Musical_Instruments' in dataset_name:
            category = 'Musical_Instruments'
        elif 'Video_Games' in dataset_name:
            category = 'Video_Games'
        elif 'CDs_and_Vinyl' in dataset_name:
            category = 'CDs_and_Vinyl'
        else:
            category = 'Musical_Instruments'
            print(f"âš ï¸ Could not determine category from {dataset_name}, using default: {category}")

        window_size = 20
        emb_token = '<answer>'
        emb_end_token = '</answer>'

        full_dataset = {
            'test': self.test_data,
            'item_info': self.item_info
        }

        self.user_gen_prompter = UserGenPrompter(
            dset=full_dataset,
            tokenizer=self.tokenizer,
            category=category,
            window_size=window_size,
            emb_token=emb_token,
            emb_end_token=emb_end_token
        )

        print("âœ… Prompter initialized")

    def build_judgment_prompt(self, user_prompt: str, candidate_item_id: int) -> str:
        """
        æ„å»ºåˆ¤æ–­promptï¼Œä¸deepseekè¯„ä¼°ä»£ç ä¸­çš„æ ¼å¼ä¿æŒä¸€è‡´
        """
        # è·å–å€™é€‰ç‰©å“ä¿¡æ¯
        item = self.item_dict.get(candidate_item_id, {})
        title = item.get('title', f'Unknown Item {candidate_item_id}')
        description = item.get('description', '')
        if isinstance(description, list):
            description = ' '.join(description[::-1]) if description else ''
        avg_rating = item.get('average_rating', '')
        num_buyers = item.get('rating_number', '')

        # æ„å»ºåˆ¤æ–­æŒ‡ä»¤
        
        # check prompt
        judgment_instruction = (
            "Please judge how well the candidate item matches the user's preferences based on the history below and the candidate item information. "
            "Produce a **single coherent paragraph** that contains the following three parts **in order**: initial reasoning, self-check/evaluation, and a final reasoning. "
            "**Mark each part by placing a single `#` immediately before its label** (for example `#Initial reasoning:`, `#Self-check:`, `#Final conclusion and suggestion:`). "
            "Do not add other `#` symbols. Please keep the entire paragraph to **roughly 400â€“800 words**. "
            "It is acceptable to draw reasonable inferences from the provided information. "
            "Finally, conclude with a clear \"yes\" or \"no\" judgment indicating whether the candidate item matches the user's preferences, enclosed within `<answer>` and `</answer>`.\n\n"
            "* `#Initial reasoning:` â€” perform a normal, thorough reasoning step that **fully considers relevant factors**.\n"
            "* `#Self-check:` â€” evaluate the initial reasoning: explicitly identify any mistakes, overlooked evidence, uncertainties, or over-weighted signals in the first pass; either confirm the initial reasoning or explain why and how you would modify it. This is a short critical reflection on the first pass.\n"
            "* `#Final conclusion and suggestion:` â€” combine the outcomes of the first two parts and present a succinct final reasoning and conclusion.\n"
        )
        
        # judgment_instruction = (
        #     f"""
        #     Please judge how well the candidate item matches the user's preferences based on the history below and the candidate item information. Produce a single coherent paragraph that contains the following two parts in order, each marked by a single `#` immediately before its label (for example `#Hypotheses:` and `#Weighing and conclusion:`). Do not add any other `#` symbols. Keep the paragraph roughly 200â€“400 words. It is acceptable to draw reasonable inferences from the provided information. Finally, conclude with a clear â€œyesâ€ or â€œnoâ€ judgment indicating whether the candidate item matches the userâ€™s preferences, enclosed within `<answer>` and `</answer>`.
        #     #Hypotheses: List 2â€“4 distinct and competing hypotheses (H1, H2, ...) about the user's preference/intent regarding this item. **You MUST include at least one hypothesis that supports a match and one that challenges it.** For each hypothesis, give a one-sentence explanation and cite the specific evidence from HISTORY or CANDIDATE ITEM that supports it (format each as `H1: ... â€” evidence: ...; H2: ... â€” evidence: ...;`).
        #     #Weighing and conclusion: Weigh the strength of the evidence for and against each hypothesis. **Explicitly compare the supporting and challenging evidence, explaining which is more definitive and why.** State which hypothesis you consider most likely and **justify this choice by acknowledging the limitations or strength of the counter-evidence.** Finally, give a final match judgement that reflects this nuanced weighing.
        #     """
        # )
        
        # normal prompt
        # judgment_instruction = (
        #     "Please judge how well the candidate item matches the user's preferences based on the history below and the candidate item information. "
        #     "Finally, conclude with a clear \"yes\" or \"no\" judgment indicating whether the candidate item matches the user's preferences, enclosed within `<answer>` and `</answer>`.\n\n"
        # )
        
        # åªä¿ç•™ "purchases and ratings (out of 5):\n" åé¢çš„éƒ¨åˆ†
        split_str = "purchases and ratings (out of 5):\n"
        if split_str in user_prompt:
            user_prompt = user_prompt.split(split_str, 1)[-1].strip()

        prompt = (
            f"{judgment_instruction}\n\n"
            f"# HISTORY:\n\n"
            f"historical musical instruments purchases and ratings (out of 5):\n{user_prompt}\n\n"
            f"# CANDIDATE ITEM:\n\n"
            f"Title: {title}\n"
            f"Average Rating: {avg_rating}\n"
            f"Number of Ratings: {num_buyers}\n"
            f"Description: {description}\n\n"
            f"# TASK:\n\n"
            f"Based on the user's history above, produce the single-paragraph output described (with parts labeled by a single `#` as explained), followed by a final yes/no judgment within `<answer>` tags.\n"
            # f"Based on the user's history above, produce the single-paragraph output described, followed by a final yes/no judgment within `<answer>` tags.\n"
        )
        
        return prompt

    def generate_prompts(self, 
                        num_samples: int = 100, 
                        match_ratio: float = 0.5) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆæŒ‡å®šæ•°é‡çš„prompté—®é¢˜
        
        Args:
            num_samples: ç”Ÿæˆçš„æ€»æ ·æœ¬æ•°
            match_ratio: åŒ¹é…æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰
        
        Returns:
            åŒ…å«promptä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        print(f"\nğŸ¯ Generating {num_samples} prompts with {match_ratio:.1%} match ratio")
        print("=" * 60)
        
        # è®¡ç®—åŒ¹é…å’Œä¸åŒ¹é…çš„æ•°é‡
        num_match = int(num_samples * match_ratio)
        num_no_match = num_samples - num_match
        
        print(f"ğŸ“Š Distribution:")
        print(f"  Match samples: {num_match}")
        print(f"  No-match samples: {num_no_match}")
        
        prompts = []
        
        # éšæœºæ‰“ä¹±æµ‹è¯•æ•°æ®ç´¢å¼•
        test_indices = list(range(len(self.test_data)))
        random.shuffle(test_indices)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        if len(test_indices) < num_samples:
            print(f"âš ï¸ Warning: Requested {num_samples} samples but only {len(test_indices)} available")
            num_samples = len(test_indices)
            num_match = int(num_samples * match_ratio)
            num_no_match = num_samples - num_match
        
        # åˆ›å»ºæ ·æœ¬ç±»å‹åˆ—è¡¨ï¼ˆTrueè¡¨ç¤ºåŒ¹é…ï¼ŒFalseè¡¨ç¤ºä¸åŒ¹é…ï¼‰
        sample_types = [True] * num_match + [False] * num_no_match
        random.shuffle(sample_types)  # éšæœºæ‰“ä¹±é¡ºåº
        
        valid_prompts = 0
        
        for i, is_match in enumerate(sample_types):
            if valid_prompts >= num_samples:
                break
                
            # è·å–æµ‹è¯•æ ·æœ¬
            sample_idx = test_indices[i]
            sample = self.test_data[sample_idx]
            
            history_ids = sample['history_item_id']
            target_id = sample['item_id']
            
            # è·³è¿‡æ— æ•ˆæ ·æœ¬
            if target_id == 0:
                continue
                
            history_ids = [id for id in history_ids if id != 0]
            history_titles = [self.id2title.get(id, f"Unknown_{id}") for id in history_ids]
            target_title = self.id2title.get(target_id, f"Unknown_{target_id}")
            
            # è·å–å¯ç”¨çš„å€™é€‰ç‰©å“ï¼ˆä¸åœ¨å†å²è®°å½•ä¸­ï¼‰
            available_items = [item_id for item_id in self.all_item_ids if item_id not in history_ids]
            if target_id not in available_items:
                print(f"Warning: Target item {target_id} is in history for sample {sample_idx}")
                continue
            
            # æ ¹æ®is_matché€‰æ‹©å€™é€‰ç‰©å“
            if is_match:
                # åŒ¹é…ï¼šä½¿ç”¨ç›®æ ‡ç‰©å“
                candidate_id = target_id
                true_label = True
            else:
                # ä¸åŒ¹é…ï¼šéšæœºé€‰æ‹©éç›®æ ‡ç‰©å“
                wrong_candidates = [item_id for item_id in available_items if item_id != target_id]
                if not wrong_candidates:
                    print(f"Warning: No wrong candidates available for sample {sample_idx}")
                    continue
                candidate_id = random.choice(wrong_candidates)
                true_label = False
            
            # ç”Ÿæˆç”¨æˆ·prompt
            sample_copy = dict(sample)
            sample_copy['seq_input_ids'] = sample_copy['history_item_id']
            sample_copy['seq_labels'] = sample_copy['item_id']
            
            try:
                chat_example = self.user_gen_prompter.to_chat_example(sample_copy)
                user_prompt = chat_example.get('prompt', '')
            except Exception as e:
                print(f"Warning: Failed to generate user prompt for sample {sample_idx}: {e}")
                continue
            
            # æ„å»ºå®Œæ•´çš„åˆ¤æ–­prompt
            full_prompt = self.build_judgment_prompt(user_prompt, candidate_id)
            
            # è·å–å€™é€‰ç‰©å“æ ‡é¢˜
            candidate_title = self.id2title.get(candidate_id, f"Unknown_{candidate_id}")
            
            # åˆ›å»ºpromptå­—å…¸
            prompt_dict = {
                'prompt_id': valid_prompts,
                'sample_id': sample_idx,
                'user_id': sample.get('user_id', 'unknown'),
                'history_item_ids': history_ids[-10:],  # åªä¿ç•™æœ€è¿‘10ä¸ª
                'history_titles': history_titles[-10:],
                'target_item_id': target_id,
                'target_title': target_title,
                'candidate_item_id': candidate_id,
                'candidate_title': candidate_title,
                'is_match': is_match,
                'user_prompt': user_prompt,
                'full_prompt': full_prompt,
                'timestamp': datetime.now().isoformat()
            }
            
            prompts.append(prompt_dict)
            valid_prompts += 1
            
            if valid_prompts % 10 == 0:
                print(f"âœ… Generated {valid_prompts}/{num_samples} prompts")
        
        print(f"\nğŸ‰ Successfully generated {len(prompts)} prompts")
        return prompts

    def save_prompts(self, prompts: List[Dict[str, Any]], output_file: str):
        """ä¿å­˜promptsåˆ°æ–‡ä»¶"""
        print(f"ğŸ’¾ Saving {len(prompts)} prompts to {output_file}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            'metadata': {
                'total_prompts': len(prompts),
                'dataset_dir': self.dataset_dir,
                'model_type': self.model_type,
                'seed': self.seed,
                'generation_time': datetime.now().isoformat(),
                'match_count': sum(1 for p in prompts if p['is_match']),
                'no_match_count': sum(1 for p in prompts if not p['is_match'])
            },
            'prompts': prompts
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Prompts saved successfully!")
        print(f"ğŸ“Š Summary:")
        print(f"  Total prompts: {save_data['metadata']['total_prompts']}")
        print(f"  Match prompts: {save_data['metadata']['match_count']}")
        print(f"  No-match prompts: {save_data['metadata']['no_match_count']}")


def main():
    parser = argparse.ArgumentParser(description="Generate configurable match/no-match binary classification prompts")
    parser.add_argument("--dataset_dir", type=str, 
                       default="/data/hongdeyao/Movies_and_TV_0_2022-10-2023-10",
                       help="Dataset directory")
    parser.add_argument("--num_samples", type=int, default=10000,
                       help="Number of prompts to generate")
    parser.add_argument("--match_ratio", type=float, default=0.5,
                       help="Ratio of match samples (0.0-1.0)")
    parser.add_argument("--model_type", type=str, default="qwen",
                       choices=["qwen", "gemma"],
                       help="Model type for prompt formatting")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed for reproducibility")
    parser.add_argument("--output_file", type=str, 
                       default="/data/hongdeyao/code/RRec/sft/data/sft_train_movies_prompts_10000.json",
                       help="Output file path")

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if not 0.0 <= args.match_ratio <= 1.0:
        print("âŒ Error: match_ratio must be between 0.0 and 1.0")
        return
    
    if not os.path.exists(args.dataset_dir):
        print(f"âŒ Error: Dataset directory not found: {args.dataset_dir}")
        return

    # åˆ›å»ºç”Ÿæˆå™¨
    generator = PromptGenerator(
        dataset_dir=args.dataset_dir,
        model_type=args.model_type,
        seed=args.seed
    )

    # ç”Ÿæˆprompts
    prompts = generator.generate_prompts(
        num_samples=args.num_samples,
        match_ratio=args.match_ratio
    )

    # ä¿å­˜prompts
    generator.save_prompts(prompts, args.output_file)

    print("\nğŸ‰ Prompt generation completed successfully!")


if __name__ == "__main__":
    main()
